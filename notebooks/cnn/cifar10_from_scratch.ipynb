{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data (from the official website http://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(indx, data):\n",
    "    imshow(np.stack(data['data'][indx].reshape(3,32,32),axis=2))\n",
    "    return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = './data/'\n",
    "NUM_TRAIN = 50000\n",
    "NUM_VAL = 5000\n",
    "NUM_TEST = 5000\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "transform_cifar = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])\n",
    "            ])\n",
    "\n",
    "# Train dataset\n",
    "cifar10_train = datasets.CIFAR10(DATA_PATH, train=True, download=True,\n",
    "                             transform=transform_cifar)\n",
    "train_loader = DataLoader(cifar10_train, batch_size=MINIBATCH_SIZE, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "#Validation set\n",
    "cifar10_val = datasets.CIFAR10(DATA_PATH, train=False, download=True,\n",
    "                           transform=transform_cifar)\n",
    "val_loader = DataLoader(cifar10_val, batch_size=MINIBATCH_SIZE, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_VAL)))\n",
    "#Test set\n",
    "cifar10_test = datasets.CIFAR10(DATA_PATH, train=False, download=True, \n",
    "                            transform=transform_cifar)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=MINIBATCH_SIZE,\n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_VAL, len(cifar10_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.261])\n",
       "           )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check if there is gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show images\n",
    "classes = test_loader.dataset.classes\n",
    "def plot_figure(image):\n",
    "    plt.imshow(np.transpose(image,(1,2,0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "rnd_sample_idx = np.random.randint(len(test_loader))\n",
    "print(f'class: {classes[test_loader.dataset[rnd_sample_idx][1]]}')\n",
    "image = test_loader.dataset[rnd_sample_idx][0]\n",
    "image = (image - image.min()) / (image.max() -image.min() )\n",
    "plot_figure(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 3, 32, 32]) tensor([6, 7, 4, 1, 8, 6, 7, 7, 1, 2, 1, 2, 8, 0, 0, 3, 7, 5, 4, 9, 7, 5, 0, 7,\n",
      "        4, 8, 5, 4, 0, 4, 8, 8, 8, 5, 3, 3, 7, 0, 3, 0, 4, 5, 2, 4, 9, 2, 0, 3,\n",
      "        4, 0, 2, 9, 6, 9, 6, 1, 4, 6, 6, 2, 2, 7, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(test_loader):\n",
    "    print(i, x.shape, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def accuracy(model, loader):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    loss = [] \n",
    "    model.eval() # no training mode, this is for batchnorm that changes depending on the mode\n",
    "    model = model.to(device=device)\n",
    "    with torch.no_grad(): # no backward required, so no computational graph is contrcuted\n",
    "        for xi, yi in loader:\n",
    "            xi = xi.to(device=device, dtype = torch.float32)\n",
    "            yi = yi.to(device=device, dtype = torch.long)\n",
    "            logits = model(xi)\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            _, pred = probabilities.max(dim=1) \n",
    "            num_correct += (pred == yi).sum() \n",
    "            num_total += pred.size(0)\n",
    "            loss.append(F.cross_entropy(input=logits, target=yi).item())\n",
    "        accuracy = float(num_correct)/num_total  \n",
    "        loss = np.array(loss).mean()\n",
    "    return loss, accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tenumerate(iterable, **kwargs):\n",
    "    _enumerate = np.ndenumerate if isinstance(iterable, np.ndarray) else enumerate\n",
    "    return _enumerate(tqdm(iterable, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model, optimiser, epochs=100):\n",
    "    model = model.to(device=device)\n",
    "    for epoch in range(epochs):\n",
    "        for i, (xi, yi) in tenumerate(train_loader):\n",
    "            model.train() # train mode\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "            # print(xi.shape)\n",
    "            #forward pass\n",
    "            logits = model(xi)\n",
    "            loss = F.cross_entropy(input= logits, target=yi)\n",
    "             #backward pass\n",
    "            optimiser.zero_grad()           \n",
    "            loss.backward()\n",
    "            #update\n",
    "            optimiser.step()              \n",
    "            #t.set_description(f'Epoch: {epoch} | loss_train: {loss.item()}')\n",
    "        loss_train, acc_train = accuracy(model, train_loader)  \n",
    "        loss_val, acc_val = accuracy(model, val_loader) \n",
    "        print(f\"Epoch: {epoch}, loss_train: {loss_train}, loss_val: {loss_val}, acc_val: {acc_val}, acc_train: {acc_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1: Linear layer\n",
    "# there is no notion of space. We just flatten the image and pass it to a sequnce of linear layers\n",
    "\n",
    "hidden_layer1 = 256\n",
    "hidden_layer2 = 256\n",
    "\n",
    "modelLinear = nn.Sequential(\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(in_features = 32 * 32 * 3, out_features=hidden_layer1), \n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(in_features=hidden_layer1, out_features=hidden_layer2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(in_features=hidden_layer2, out_features=10),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "optimiser = torch.optim.Adam(modelLinear.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:21<00:00, 36.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss_train: 1.7892625809020704, loss_val: 1.8129648483252223, acc_val: 0.3836, acc_train: 0.39458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:23<00:00, 33.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss_train: 1.6586224177609319, loss_val: 1.7203613082064857, acc_val: 0.4116, acc_train: 0.43542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:21<00:00, 36.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss_train: 1.604919583596232, loss_val: 1.7047699222081825, acc_val: 0.4268, acc_train: 0.45144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:20<00:00, 37.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, loss_train: 1.4921675812252952, loss_val: 1.6474935963184019, acc_val: 0.4468, acc_train: 0.49576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:20<00:00, 38.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss_train: 1.3372367700499952, loss_val: 1.5224562566491622, acc_val: 0.4816, acc_train: 0.53944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:20<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, loss_train: 1.1773270016436077, loss_val: 1.4229916017266768, acc_val: 0.518, acc_train: 0.59466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:20<00:00, 37.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, loss_train: 1.0881379415921848, loss_val: 1.3924375425411175, acc_val: 0.52, acc_train: 0.62244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:20<00:00, 37.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, loss_train: 1.022408722764086, loss_val: 1.4152805458141278, acc_val: 0.5248, acc_train: 0.64404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:22<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, loss_train: 0.9580243517980551, loss_val: 1.4011329450184786, acc_val: 0.5352, acc_train: 0.67062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:20<00:00, 38.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, loss_train: 0.912553585551279, loss_val: 1.451909173138534, acc_val: 0.5346, acc_train: 0.68808\n"
     ]
    }
   ],
   "source": [
    "train(modelLinear, optimiser=optimiser, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def resize_image(image: Image, length: int) -> Image:\n",
    "    if image.size[0] < image.size[1]:\n",
    "        # The image is in portrait mode. Height is bigger than width.\n",
    "        resized_image = image.resize((length, int(image.size[1] * (length / image.size[0]))))\n",
    "        # Amount of pixel to lose in total on the height of the image.\n",
    "        required_loss = (resized_image.size[1] - length)\n",
    "        # Crop the height of the image so as to keep the center part.\n",
    "        resized_image = resized_image.crop(\n",
    "            box=(0, required_loss / 2, length, resized_image.size[1] - required_loss / 2))\n",
    "        # We now have a length*length pixels image.\n",
    "        return resized_image\n",
    "    else:\n",
    "        # This image is in landscape mode or already squared. The width is bigger than the heihgt.\n",
    "        # This makes the height fit the LENGTH in pixels while conserving the ration.\n",
    "        resized_image = image.resize((int(image.size[0] * (length / image.size[1])), length))\n",
    "        # Amount of pixel to lose in total on the width of the image.\n",
    "        required_loss = resized_image.size[0] - length\n",
    "        # Crop the width of the image so as to keep 1080 pixels of the center part.\n",
    "        resized_image = resized_image.crop(\n",
    "            box=(required_loss / 2, 0, resized_image.size[0] - required_loss / 2, length))\n",
    "        # We now have a length*length pixels image.\n",
    "        return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# img = resize_image(img, 32)\n",
    "# imshow(img)\n",
    "def inference(model, url):\n",
    "  \n",
    "    urllib.request.urlretrieve(url, 'tmp.png')\n",
    "    img = Image.open(\"tmp.png\")\n",
    "    img = resize_image(img, 32)\n",
    "    im = transforms.ToTensor()(img)\n",
    "\n",
    "    model.eval() # evaluation mode\n",
    "    x = im.to(device=device, dtype=torch.float32)\n",
    "    x = x.view(1,3,32,32)\n",
    "    imshow(x[0,0])\n",
    "    # print(x.shape)\n",
    "    #forward pass\n",
    "    with torch.no_grad(): \n",
    "        logits = model(x)\n",
    "        # print(logits.shape)\n",
    "        probs = F.softmax(logits, dim=-1)[0] # we convert it into a probability\n",
    "        guess = torch.argmax(probs)\n",
    "    # print(probs)\n",
    "    print(f\"it's a {classes[guess.item()]} with {probs[guess.item()]:.2f} probability\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential CNN\n",
    "\n",
    "ch1 = 16 \n",
    "ch2 = 32 \n",
    "\n",
    "modelCNN1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=3, out_channels=ch1, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(in_channels=ch1,out_channels=ch2,kernel_size=3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(in_features=16*16*ch2,out_features=10),\n",
    "                        )\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "optimiser = torch.optim.Adam(modelCNN1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:56<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss_train: 2.303312462918899, loss_val: 2.3033143719540368, acc_val: 0.0994, acc_train: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:55<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss_train: 2.3032579623219913, loss_val: 2.3025927966154076, acc_val: 0.1014, acc_train: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:50<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss_train: 2.302750750575834, loss_val: 2.3028022699718234, acc_val: 0.0994, acc_train: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:01<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, loss_train: 2.303299252334458, loss_val: 2.3034218474279475, acc_val: 0.0976, acc_train: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:53<00:00, 14.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss_train: 2.3035043118250034, loss_val: 2.3035105964805505, acc_val: 0.1008, acc_train: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:50<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, loss_train: 2.3033100485496814, loss_val: 2.3035341697403147, acc_val: 0.1026, acc_train: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 670/782 [00:45<00:07, 14.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(modelCNN1, optimiser\u001b[39m=\u001b[39;49moptimiser, epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimiser, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m yi \u001b[39m=\u001b[39m yi\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m      9\u001b[0m \u001b[39m# print(xi.shape)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logits \u001b[39m=\u001b[39m model(xi)\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m logits, target\u001b[39m=\u001b[39myi)\n\u001b[1;32m     13\u001b[0m  \u001b[39m#backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ml/notebooks/cnn/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/ml/notebooks/cnn/env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Projects/ml/notebooks/cnn/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/ml/notebooks/cnn/env/lib/python3.9/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/Projects/ml/notebooks/cnn/env/lib/python3.9/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(modelCNN1, optimiser=optimiser, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's a dog with 0.69 probability\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_url = 'https://ggsc.s3.amazonaws.com/images/uploads/The_Science-Backed_Benefits_of_Being_a_Dog_Owner.jpg'  \n",
    "inference(modelCNN1, image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
