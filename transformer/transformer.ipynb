{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Implementation of the original paper \"Attention is All You Need\" https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "We will consider the task of translating Catalan to English.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Embedding\n",
    "Consider a vocabulary of `vocab_size` words. Each of them will have assign an integer. So the first thing we need is to encode a sentence by asigning these integers. This is what is called **tokenize**.  Then to each of this numbers we will assing a (learnable) vector of size a certain size, in the paper it is called $d_{\\rm model}$.\n",
    "\n",
    "In PyTorch we can use an `nn.Embedding` class to get this. It's simply a matrix of size `vocab_size` x `d_model`, and it simply selects the row that corresponds to the integer of the word (the total of words is `vocab_size`) and outputs the vector of size `d_model`. So for a sentence of size `context_size`, the output of the embedding is a tensor of size `context_size`x`d_model`.\n",
    "\n",
    "In the paper they multiply the embedding output by $\\sqrt{d_{\\rm model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Input Embedding\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model : int, vocab_size : int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * np.sqrt(self.d_model) # multiplied by sqrt(d_model) in the paper\n",
    "\n",
    "# Test\n",
    "seq_len = 10 # The lenght of the input sequence\n",
    "input_embedding = InputEmbedding(d_model=768, vocab_size=10)\n",
    "x = torch.randint(0, seq_len, (5, 10)) # (N, context_size)\n",
    "out = input_embedding(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional Encoding\n",
    "\n",
    "A Transformer has no knowledge about positions of the input sequence. We define a context length, that will be the maximum input for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x723e2f1ef650>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Positional Encoding\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "def sinusoidal_encoding(context_length, d_model):\n",
    "    pe = torch.zeros(context_length, d_model)\n",
    "    position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1) # the unsqueeze converts it to a (context_length, 1) tensor\n",
    "    div_term = torch.exp( torch.arange(0, d_model, 2, dtype=torch.float) * (- np.log(10000.0)) / d_model) # (d_model/2)\n",
    "    # position * div_term = (context_lenght, emb_dim/2)\n",
    "\n",
    "    pe[:, ::2] = torch.sin(position * div_term) # sin for even positions\n",
    "    pe[:, 1::2] = torch.cos(position * div_term) # cos for odd positions\n",
    "\n",
    "    pe = pe.unsqueeze(0) # (1, context_length, d_model)\n",
    "    return pe  \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model : int, context_length : int, dropout : float):\n",
    "       super().__init__() \n",
    "       self.d_model = d_model\n",
    "       self.context_length = context_length\n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "       self.register_buffer('positional_encoding', sinusoidal_encoding(self.context_length, self.d_model)) # (1, context_length, d_model ))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "       x = x + (self.positional_encoding[:, :x.shape[1], :]).requires_grad_(False) # only the text we have\n",
    "       return self.dropout(x) # (batch, seq_len, d_model)\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "sin_enc = sinusoidal_encoding(context_length=10, d_model=128)\n",
    "print(sin_enc.shape)\n",
    "plt.imshow(sin_enc[0].numpy(), aspect='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer Norm\n",
    "From the paper https://arxiv.org/pdf/1607.06450\n",
    "\n",
    "If we have a batch of size N, then for item in the batch, let's say sentences of seq_len size, then we compute the mean and the std of each one and normalize the element as\n",
    "\n",
    "$$ x_i' = \\gamma \\dfrac{x_i - \\mu_i}{\\sqrt{\\sigma_j^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Layer Norm\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# TODO: check the mean(dim=-1). The sequence dimension is the dim = 1, no?\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    # is basically a batch norm (less complicated) across sequence dimension\n",
    "    def __init__(self, epsilon : float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon # For numerical stability\n",
    "        self.gamma = nn.Parameter(torch.ones(1)) # Learnable parameter (multiplicative parameter)\n",
    "        self.beta = nn.Parameter(torch.zeros(1)) # Learnable parameter (bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdims=True)  # (batch, seq_len, 1)\n",
    "        std = x.std(dim=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed Forward Block\n",
    "\n",
    "In additino to the attention layers, tey add fully connected feed-forward network applied to each position separately and identically. This consists of two linear transformations with a ReLU in between (Eq. 2 of the paper)\n",
    "$$ \\rm{FFN}(x) = max(0, x W_1 + b_1) W_2 + b_2 $$\n",
    "THe input and output has dimension $d_{\\rm model}$ and the inner-layer $d_{ff}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Feed Forward\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout : float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff1 = nn.Linear(d_model, d_ff)\n",
    "        self.ff2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ff1(x)) # (batch, seq_len, d_model) -> (batch, seq_len, d_ff)\n",
    "        x = self.dropout(x) \n",
    "        x = self.ff2(x) # (batch, seq_len, d_ff) -> (batch, seq_len, d_ff)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Head Attention\n",
    "\n",
    "- Section 3.2 of the paper.\n",
    "- *TODO:* Check the formulas.\n",
    "- Here we use the same dimension $d_k$ for the keys, queries and **also** for the values, although they could be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Muli-Head Attention\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# 1. The multihead attention dividies the embedding dimension into multiple smaller attentions. \n",
    "# 2. The number of heads must divide the input dimension (the sequence one)\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model : int, nheads : int, dropout : float):\n",
    "        super().__init__()\n",
    "        assert d_model % nheads == 0, \"nheads must divide d_model\" # check that d_model can be divided by the num of heads\n",
    "        self.emb_dim = d_model\n",
    "        self.nheads = nheads\n",
    "        self.dk = d_model // nheads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "\n",
    "   \n",
    "    @staticmethod\n",
    "    def attention(query, key, values, mask, dropout: nn.Dropout):\n",
    "        dk = query.shape[-1]\n",
    "\n",
    "        # Mulitply queries and keys\n",
    "        attention_scores = query @ key.transpose(-1, -2) / np.sqrt(dk) # (batch, nhead, seq_len, dk) @  (batch, nhead, dk, seq_len) -> (batch, nhead, seq_len, seq_len)\n",
    "        \n",
    "        # If mask is defined, then apply it. Basically, all the indices that in the mask are zero, then replace it with -inf\n",
    "        if mask is not None: attention_scores.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "        # Apply softmax (the -inf will be 0)\n",
    "        attention_scores = attention_scores.softmax(dim=1) # sofmax each row (B, nhead, seq, seq)\n",
    "        \n",
    "        # Apply dropout\n",
    "        if dropout is not None: attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # Finally multiply by the values\n",
    "        x = attention_scores @ values # (batch, nhead, seq_len, seq_len) @ (batch, nhead, seq_len, dk) -> (batch, nhead, seq_len, dk)\n",
    "\n",
    "        return x, attention_scores # We also return te attention scores for later visualization\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        \n",
    "        qi = self.Wq(q) # (batch, seq_len, d_model) @ (batch, d_model, d_model) -> (batch, seq_len, d_model)\n",
    "        ki = self.Wk(k) \n",
    "        vi = self.Wv(v) \n",
    "\n",
    "        # Split across the embedding dimension and rearange.\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, nhead, dk = d_model/nheads) ->  (batch, nhead, seq_len, dk)\n",
    "        qi = qi.view(qi.shape[0],  qi.shape[1], self.nheads, self.dk).transpose(1,2) # (batch, nhead, seq_len, dk = d_model/nheads)\n",
    "        ki = ki.view(ki.shape[0],  ki.shape[1], self.nheads, self.dk).transpose(1,2)\n",
    "        vi = vi.view(vi.shape[0],  vi.shape[1], self.nheads, self.dk).transpose(1,2)\n",
    "\n",
    "        # Calculate attention \n",
    "        x, self.attention_scores = self.attention(qi, ki, vi, mask, self.dropout)  # (batch, nhead, seq_len, dk) \n",
    "        \n",
    "        # Concat the heads\n",
    "        # (batch, nhead, seq_len, dk) -> (batch, seq_len, nheads, dk) -> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.nheads * self.dk) # (batch, seq_len, d_model). \n",
    "\n",
    "        # Apply a last linear layer\n",
    "        x = self.Wo(x) # (batch, seq_len, d_model) @ (d_model, d_model) -> (batch, seq_len, d_model)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residual Connection\n",
    "\n",
    "We need a skip conection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout : float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # Slightly different from the original paper, we first apply the norm, then the sublayer\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder Layer\n",
    "An encoder layer will have a Mult-Head Attention block and a Feed-Forward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, nheads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(d_model=d_model, nheads=nheads, dropout=dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, source_mask):\n",
    "        # The first residual connection\n",
    "        x = self.residual_connections[0](x, lambda x : self.self_attention(x, x, x, source_mask))\n",
    "        # The second residual conection \n",
    "        x = self.residual_connections[1](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder\n",
    "\n",
    "The encoder will consist on $N$ encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, nheads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(d_model, nheads, dropout)\n",
    "        self.cross_attention = MultiHeadAttentionBlock(d_model, nheads, dropout) \n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        # First residual connection (self-attention for the decoder)\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, target_mask)) # mask for the decoder\n",
    "        # Second residual connection (cross-attention, the queries come from the decoder, keys and values come from the encoder)\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention(x, encoder_output, encoder_output, source_mask))\n",
    "        # Thrid residual connection (feed-forward)\n",
    "        x = self.residual_connections[2](x, self.feed_forward)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, source_mask, target_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Layer \n",
    "\n",
    "1. This last layer will convert the embedding layer to the sequence.\n",
    "2. The ouput will be the log probabilities (logits) for each input what is the corresponding out word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module): # convert from the embedding to the vocabulary\n",
    "    def __init__(self, d_model : int, out_vocab_size : int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "\n",
    "        self.linear = nn.Linear(d_model, out_vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x) # (batch, seq_len, d_model) @ (d_model, out_vocab_size) -> (batch, seq_len, out_vocab_size)j\n",
    "        return x # return the logits\n",
    "\n",
    "        # x = F.log_softmax(x, dim=-1) # we return the log softmax, then exponentiate to get probablities, more stable??\n",
    "        # return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer\n",
    "Combine everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 source_context_length : int, \n",
    "                 target_context_length : int, \n",
    "                 d_model : int, \n",
    "                 nlayers : int, \n",
    "                 d_ff : int, \n",
    "                 nheads : int,\n",
    "                 source_vocab_size : int,\n",
    "                 target_vocab_size : int,\n",
    "                 dropout : float):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Hyperparams\n",
    "        self.d_model = d_model\n",
    "        self.source_context_length = source_context_length\n",
    "        self.target_context_length = target_context_length\n",
    "        self.nlayers = nlayers\n",
    "        self.d_ff = d_ff\n",
    "        self.nheads = nheads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(nn.ModuleList([EncoderLayer(d_model, d_ff, nheads, dropout) for _ in range(nlayers)]))\n",
    "        self.source_input_embedding = InputEmbedding(d_model, source_vocab_size)\n",
    "        self.source_positional_encoding = PositionalEncoding(d_model, source_context_length, dropout)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(nn.ModuleList([DecoderLayer(d_model, d_ff, nheads, dropout) for _ in range(nlayers)]))\n",
    "        self.target_input_embedding = InputEmbedding(d_model, target_vocab_size)\n",
    "        self.target_positional_encoding = PositionalEncoding(d_model, target_context_length, dropout)\n",
    "\n",
    "        # Last linear layer\n",
    "        self.linear = LinearLayer(d_model, target_vocab_size)\n",
    "\n",
    "        # Init the parameters after building all the blocks\n",
    "        self._init_parameters()\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        source = self.source_input_embedding(source)\n",
    "        source = self.source_positional_encoding(source)\n",
    "        return self.encoder(source, source_mask)\n",
    "\n",
    "    def decode(self, target, encoder_output, source_mask, target_mask):\n",
    "        target = self.target_input_embedding(target)\n",
    "        target = self.target_positional_encoding(target)\n",
    "        return self.decoder(target, encoder_output, source_mask, target_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    # Initiliazion of the parameters, as in the paper. I think PyTorch does it automatically, because all are linear layers. \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, encoder_mask, decoder_mask):\n",
    "        encoder_ouput = self.encode(encoder_input, encoder_mask)  \n",
    "        decoder_ouput = self.decode(decoder_input, encoder_ouput, encoder_mask, decoder_mask) \n",
    "\n",
    "        logits = self.project(decoder_ouput) \n",
    "        return logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of the paper\n",
    "paper_config = {\n",
    "    'd_model': 512,\n",
    "    'nlayers' : 6,\n",
    "    'nheads' : 8,\n",
    "    'dropout' : 0.1,\n",
    "    'd_ff' : 2048,\n",
    "    'seq_len': 300, # more than enough\n",
    "    'source_lang': 'ca',\n",
    "    'target_lang': 'en',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training\n",
    "\n",
    "**Encoder**\n",
    "- The input of the encoder is the sentence in catalan. In the form of \n",
    "        \n",
    "        SOS (sentence in Catalan) EOS \n",
    "with the two special tokens Start Of Sentence and End Of Sentence. \n",
    "- The input will have a dimension of (batch, seq_len, d_model), before we had (seq_len, source_vocab_size) but with the embedding we convert it to (seq_len, d_model). \n",
    "- The output will have the same dimensions. For each word, the ouput vector captures not only the meaning (embedding) or the position, but also the interaction with other words \n",
    "- To have the same seq_len for all sentences in a batch, we pad them with a special token PAD.\n",
    "\n",
    "**Decoder** \n",
    "- The input of the decoder will be the sentence in the form \n",
    "\n",
    "        SOS (sentence in English)\n",
    "without the EOS token. It will have dimensions of (batch, seq_len, d_model).\n",
    "- We prepend the SOS token at the beginning, that's why in the paper says that the decoder input is shifted to the right\n",
    "- To have the same seq_len for all sentences in a batch, we pad them with a special token PAD.\n",
    "- The decoder ouput will have dimensions (batch, seq_len, d_model)\n",
    "- The output of the decoder wil be fed to the last linear layer to transform it to (seq_len, target_vocab_size). \n",
    "- Apply softmax to get the probabilties for each output token\n",
    "\n",
    "**The target**\n",
    "- The target will of the form: \n",
    "        \n",
    "        (sentence in English) EOS \n",
    "without the SOS token. We will compute a Cross Entropy Loss between the probabilites and the this target.\n",
    "\n",
    "**Why the special tokens**\n",
    "\n",
    "Let us consider we want to translate the sentence: \n",
    "        \n",
    "        SOS La casa vermella es gran EOS\n",
    "\n",
    "During the training, the input of the decoder will be\n",
    "\n",
    "        SOS The red house is big\n",
    "\n",
    "The target will be:\n",
    "\n",
    "        The red house is big EOS\n",
    "\n",
    "Now, the idea is that \n",
    "        \n",
    "- when we have a *SOS* -> we want to ouput a *The*\n",
    "- *The* -> *red*\n",
    "- *red* -> *house*\n",
    "- *house* -> *is*\n",
    "- *is* -> *big*\n",
    "- *big* -> *EOS*\n",
    "\n",
    "This is just one time step. And we have done it everything in **one** time step. That is the power of the Transformer. That's the advantaged over the RNN, that to process N tokens we need N time steps.\n",
    "\n",
    "### Inference\n",
    "\n",
    "- The input of the encoder is the same, with (seq_len, d_model)\n",
    "\n",
    "- The input of the decoder will be just the SOS token (and the padding to have the same lenght)\n",
    "- The ouput of the decoder will be the logits, that after the softmax will be probabiliteis assign to each token of the vocabulary. We take the token with the maximum probability, and this will be the next predicted token.\n",
    "\n",
    "This is one time step. The next, is to use the same encoder output (there is no need to recompute it again). Use the output of the decoder as the input of the decoder. Do the same until we see the EOS in the decoder output.\n",
    "\n",
    "**Notes**\n",
    "- What we have used for the inference is called *greedy strategy*. At each step, we select the word with the maximum softmax value.\n",
    "- There is another strategy, called *beam search*. At each step, we select the top B words and evaluate all the possible next words for each of them and at each step, keeping the top B most probable sequences. This usually performs better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "We will use the Opus Books dataset, from catalan to english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The tokenizer\n",
    "\n",
    "- We need to set a tokenizer. There are **lots** of several ways to tokenize a sentence. OpenAI GPT models for instance use tokenizer that encode sylables or more words.\n",
    "- We will use a tokenizer that encodes each word separatly.\n",
    "- We will use the Hugging Face library ``tokenizer``. At some point we can be our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>translation.ca</th>\n",
       "      <th>translation.en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Les Aventures De Tom Sawyer</td>\n",
       "      <td>The Adventures of Tom Sawyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>PREFACI.</td>\n",
       "      <td>PREFACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>La major part de les aventures consignades en ...</td>\n",
       "      <td>Most of the adventures recorded in this book r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Huck Finn és tret de la vida real; Tom Sawyer ...</td>\n",
       "      <td>Huck Finn is drawn from life; Tom Sawyer also,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>4600</td>\n",
       "      <td>Així acaba aquesta cronica.</td>\n",
       "      <td>SO endeth this chronicle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>4601</td>\n",
       "      <td>Com que és estrictament la historia d'un minyó...</td>\n",
       "      <td>It being strictly a history of a _boy_, it mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>4602</td>\n",
       "      <td>Quan hom escriu una novel·la sobre gent gran, ...</td>\n",
       "      <td>When one writes a novel about grown people, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>4603</td>\n",
       "      <td>La major part dels personatges que surten en a...</td>\n",
       "      <td>Most of the characters that perform in this bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>4604</td>\n",
       "      <td>Podra semblar, algun dia, que valgui la pena d...</td>\n",
       "      <td>Some day it may seem worth while to take up th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4604 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                     translation.ca  \\\n",
       "0        1                        Les Aventures De Tom Sawyer   \n",
       "1        2                                         Mark Twain   \n",
       "2        3                                           PREFACI.   \n",
       "3        4  La major part de les aventures consignades en ...   \n",
       "4        5  Huck Finn és tret de la vida real; Tom Sawyer ...   \n",
       "...    ...                                                ...   \n",
       "4599  4600                        Així acaba aquesta cronica.   \n",
       "4600  4601  Com que és estrictament la historia d'un minyó...   \n",
       "4601  4602  Quan hom escriu una novel·la sobre gent gran, ...   \n",
       "4602  4603  La major part dels personatges que surten en a...   \n",
       "4603  4604  Podra semblar, algun dia, que valgui la pena d...   \n",
       "\n",
       "                                         translation.en  \n",
       "0                          The Adventures of Tom Sawyer  \n",
       "1                                            Mark Twain  \n",
       "2                                               PREFACE  \n",
       "3     Most of the adventures recorded in this book r...  \n",
       "4     Huck Finn is drawn from life; Tom Sawyer also,...  \n",
       "...                                                 ...  \n",
       "4599                          SO endeth this chronicle.  \n",
       "4600  It being strictly a history of a _boy_, it mus...  \n",
       "4601  When one writes a novel about grown people, he...  \n",
       "4602  Most of the characters that perform in this bo...  \n",
       "4603  Some day it may seem worth while to take up th...  \n",
       "\n",
       "[4604 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download it from https://huggingface.co/datasets/Helsinki-NLP/opus_books/tree/main/ca-en\n",
    "# !wget https://huggingface.co/datasets/Helsinki-NLP/opus_books/blob/main/ca-en/train-00000-of-00001.parquet\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_parquet('./data/opus_books_ca_en.parquet', engine='fastparquet').drop(index=0).reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokenizer from ./tokenizer_ca.json\n",
      "Getting tokenizer from ./tokenizer_en.json\n",
      "encoded sentence: [1458, 11, 1078]\n",
      "decoded sequence: Hola que tal\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "def get_all_sentences(dataset, lang):\n",
    "    for item in dataset[f'translation.{lang}']:\n",
    "        yield item\n",
    "\n",
    "# Check https://huggingface.co/docs/tokenizers/pipeline\n",
    "def get_tokenizer(tokenizer_path, dataset, lang):\n",
    "    if not Path.exists(Path(tokenizer_path)):\n",
    "        print(f\"The tokenizer with path {tokenizer_path} does not exists, creating a new one\")\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        print(f\"Getting tokenizer from {tokenizer_path}\")\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "# Get the tokenizers\n",
    "tokenizer_source = get_tokenizer(\"./tokenizer_ca.json\", dataset, 'ca')\n",
    "tokenizer_target = get_tokenizer(\"./tokenizer_en.json\", dataset, 'en')\n",
    "\n",
    "# Test\n",
    "tokens_source = tokenizer_source.encode(\"Hola que tal\").ids\n",
    "print(f\"encoded sentence: {tokens_source}\")\n",
    "print(f\"decoded sequence: {tokenizer_source.decode(tokens_source)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for retrieving data from\n",
    "\n",
    "def causal_mask(size):\n",
    "    # This create a matrix of size x size with the lower diagonal filled with ones (including the diagonal). \n",
    "    # This makes that tokens can only interact with past tokens and themselfs\n",
    "    mask = torch.tril(torch.ones(1, size, size)).type(torch.int)  == 1 \n",
    "    return mask \n",
    "\n",
    "class BilingualDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, tokenizer_source, tokenizer_target, source_lang, target_lang, context_length):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer_source = tokenizer_source\n",
    "        self.tokenizer_target = tokenizer_target\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.context_length = context_length\n",
    "\n",
    "        special_tokens = ['[SOS]', '[EOS]', '[PAD]']\n",
    "        self.special_tokens = { token:torch.tensor([tokenizer_source.token_to_id(token)], dtype = torch.int64) for token in special_tokens} \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        element = self.dataset.loc[index]\n",
    "        source_text = element[f'translation.{self.source_lang}']          \n",
    "        target_text = element[f'translation.{self.target_lang}']          \n",
    "\n",
    "        source_tokens = tokenizer_source.encode(source_text).ids\n",
    "        target_tokens = tokenizer_target.encode(target_text).ids\n",
    "\n",
    "        # In a batch, the sentences will have different lenghts, that's why we need to pad the, using the [PAD] token\n",
    "        encoder_input_padding = self.context_length - len(source_tokens) - 2 # take into account the [SOS] and [EOS] ones\n",
    "        decoder_input_padding = self.context_length - len(target_tokens) - 1 # take into account only the [EOS] token, since the input of the decoder is already the [SOS] token\n",
    "\n",
    "        if encoder_input_padding < 0 or decoder_input_padding < 0 :\n",
    "            raise ValueError(\"Sentence is too long\") \n",
    "        \n",
    "        encoder_input = torch.cat( [\n",
    "                self.special_tokens['[SOS]'],\n",
    "                torch.tensor(source_tokens, dtype=torch.int64),\n",
    "                self.special_tokens['[EOS]'],\n",
    "                torch.tensor([self.special_tokens['[PAD]']]*encoder_input_padding, dtype=torch.int64)\n",
    "            ])\n",
    "        \n",
    "        # Adding SOS to the decoder input\n",
    "        decoder_input = torch.cat([\n",
    "                self.special_tokens['[SOS]'],\n",
    "                torch.tensor(target_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.special_tokens['[PAD]']]*decoder_input_padding, dtype=torch.int64)\n",
    "            ])\n",
    "\n",
    "        # Adding EOS to labels \n",
    "        labels = torch.cat([\n",
    "                torch.tensor(target_tokens, dtype=torch.int64),\n",
    "                self.special_tokens['[EOS]'],\n",
    "                torch.tensor([self.special_tokens['[PAD]']]*decoder_input_padding, dtype=torch.int64)\n",
    "            ])\n",
    "\n",
    "        assert encoder_input.shape[0] == self.context_length\n",
    "        assert decoder_input.shape[0] == self.context_length\n",
    "        assert labels.shape[0] == self.context_length\n",
    "\n",
    "        return { \n",
    "            'encoder_input':encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.special_tokens['[PAD]']).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.special_tokens['[PAD]']).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.shape[0]),\n",
    "            'labels': labels,\n",
    "            'source_text': source_text,\n",
    "            'target_text': target_text\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_set)=4143, len(val_set)=461\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "torch.manual_seed(1234)\n",
    "random_indices = torch.randperm(len(dataset))\n",
    "train_ratio = 0.9\n",
    "val_ratio = 1 - train_ratio\n",
    "\n",
    "train_set = dataset.iloc[random_indices[:int(train_ratio*len(dataset))]].reset_index(drop=True)\n",
    "val_set = dataset.iloc[random_indices[int(train_ratio*len(dataset)):]].reset_index(drop=True)\n",
    "\n",
    "print(f\"{len(train_set)=}, {len(val_set)=}\")\n",
    "\n",
    "context_length = 100\n",
    "\n",
    "train_dataset =  BilingualDataset(train_set, tokenizer_source=tokenizer_source, tokenizer_target=tokenizer_target, source_lang='ca', target_lang='en', context_length=context_length)\n",
    "\n",
    "val_dataset =  BilingualDataset(train_set, tokenizer_source=tokenizer_source, tokenizer_target=tokenizer_target, source_lang='ca', target_lang='en', context_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_splits(dataset, train_ratio, source_lang, target_lang, context_length, seed=None):\n",
    "\n",
    "    # Get the tokenizers\n",
    "    tokenizer_source = get_tokenizer(f\"./tokenizer_{source_lang}.json\", dataset, source_lang)\n",
    "    tokenizer_target = get_tokenizer(f\"./tokenizer_{target_lang}.json\", dataset, target_lang)\n",
    "\n",
    "    # Check the maximum source and target lengths\n",
    "    max_len_source, max_len_target = 0, 0\n",
    "    for i, row in dataset.iterrows():\n",
    "        source_ids = tokenizer_source.encode(row[f\"translation.{source_lang}\"]).ids\n",
    "        target_ids = tokenizer_target.encode(row[f\"translation.{target_lang}\"]).ids\n",
    "        max_len_source = max(max_len_source, len(source_ids)) \n",
    "        max_len_target = max(max_len_target, len(target_ids)) \n",
    "\n",
    "    print(f\"Maximum length of source sentence: {max_len_source}\")\n",
    "    print(f\"Maximum length of target sentence: {max_len_target}\")\n",
    "    assert context_length >= max_len_source and context_length >= max_len_target\n",
    "\n",
    "    if seed != None : torch.manual_seed(seed)\n",
    "    random_indices = torch.randperm(len(dataset))\n",
    "    print(random_indices)\n",
    "    train_set = dataset.iloc[random_indices[:int(train_ratio*len(dataset))]].reset_index(drop=True)\n",
    "    val_set = dataset.iloc[random_indices[int(train_ratio*len(dataset)):]].reset_index(drop=True)\n",
    "    print(f\"{len(train_set)=}, {len(val_set)=}\")\n",
    "\n",
    "\n",
    "    train_dataset =  BilingualDataset(train_set, tokenizer_source=tokenizer_source, tokenizer_target=tokenizer_target, source_lang=source_lang, target_lang=target_lang, context_length=context_length)\n",
    "\n",
    "    val_dataset =  BilingualDataset(train_set, tokenizer_source=tokenizer_source, tokenizer_target=tokenizer_target, source_lang=source_lang, target_lang=target_lang, context_length=context_length)\n",
    "\n",
    "    return train_dataset, val_dataset, tokenizer_source, tokenizer_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tokenizer from ./tokenizer_ca.json\n",
      "Getting tokenizer from ./tokenizer_en.json\n",
      "Maximum length of source sentence: 220\n",
      "Maximum length of target sentence: 210\n",
      "tensor([ 719,   92, 1802,  ..., 1116, 3374, 3614])\n",
      "len(train_set)=4143, len(val_set)=461\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, tokenizer_source, tokenizer_target = get_dataset_splits(dataset, 0.9, 'ca', 'en', 250 ,1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3927, 4592)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_target.get_vocab_size(), tokenizer_source.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#\n",
    "# Prepare the training\n",
    "#\n",
    "# -------------------------\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "def CFG(text, color): return \"\\33[38;5;\" + str(color) + \"m\" + text + \"\\33[0m\"\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  The model parameters\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "# Configuration of the paper\n",
    "model_config = {\n",
    "    'd_model': 512,\n",
    "    'nlayers' : 6,\n",
    "    'nheads' : 8,\n",
    "    'dropout' : 0.1,\n",
    "    'd_ff' : 2048,\n",
    "    'source_context_length': 300, # more than enough\n",
    "    'target_context_length': 300, # more than enough\n",
    "    'source_vocab_size': tokenizer_source.get_vocab_size(),\n",
    "    'target_vocab_size': tokenizer_target.get_vocab_size()\n",
    "}\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Transformer(**model_config)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Get data\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# initialize the dataloaders after re-seed\n",
    "torch.manual_seed(0)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1) \n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "# For now we do it like that, not the ideal, because we are creating an iterator at each call\n",
    "def get_data(split):\n",
    "    iterator = iter(train_dataloader) if split == 'train' else iter(val_dataloader)\n",
    "    batch = next(iterator)\n",
    "    return batch\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Loss function\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_source.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Compute model_metrics\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "def compute_model_metrics(batch):\n",
    "    \n",
    "    encoder_input = batch['encoder_input'].to(device) # (batch, context_length)\n",
    "    decoder_input = batch['decoder_input'].to(device) # (batch, context_length)\n",
    "    encoder_mask = batch['encoder_mask'].to(device) # (batch, 1, 1, context_length)\n",
    "    decoder_mask = batch['decoder_mask'].to(device) # (batch, 1, context_length, context_length)\n",
    "    labels = batch['labels'].to(device) # (batch, context_lenght)\n",
    "\n",
    "    logits = model.forward(encoder_input, decoder_input, encoder_mask,decoder_mask) # (batch, context_length, target_vocab_size)\n",
    "\n",
    "    # reshape the logits and the labels to feed into the loss function\n",
    "    logits  = logits.view(-1, tokenizer_target.get_vocab_size()) # (batch * context, vocab_size)\n",
    "    # labels = F.one_hot(labels.view(-1), num_classes=tokenizer_target.get_vocab_size())\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(logits, labels)\n",
    "\n",
    "    return {'loss': loss}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Learning rate schedule\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "def get_lr(it):\n",
    "    return learning_rate\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Optimizer\n",
    "#\n",
    "# ---------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-9)\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Configure how to evaluate the metrics\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "metrics_to_eval = ['loss'] # they have to be returned from the method forward_batch() of the model\n",
    "eval_iters = 5\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_metrics():\n",
    "    out_dicts = {s: {} for s in metrics_to_eval}\n",
    "    model.eval()  # set the model to eval mode\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        max_iters = eval_iters\n",
    "        local_dicts = {s: torch.zeros(max_iters) for s in metrics_to_eval}\n",
    "        for k in tqdm(range(max_iters), file=sys.stdout, desc=f\"{CFG('[train]',11)}\" if split == \"train\" else f\"{CFG('[ val ]',11)}\"):\n",
    "            batch = get_data(split)\n",
    "            fb = compute_model_metrics(batch)\n",
    "            for metric in metrics_to_eval:\n",
    "                local_dicts[metric][k] = fb[metric].item()\n",
    "\n",
    "        for metric in metrics_to_eval:\n",
    "            out_dicts[metric][split] = local_dicts[metric].mean()\n",
    "\n",
    "    model.train()  # set the model back to train mode\n",
    "    return out_dicts\n",
    "\n",
    "best_val_loss = 1e9\n",
    "info = { 'iter_num': [], 'lr': [],  **{ m : {'train': [], 'val': []} for m in metrics_to_eval}}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#\n",
    "#  Visualize during training\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "def greedy_decode(model, encoder_input, encoder_mask, tokenizer_source, max_length ):\n",
    "    sos_idx = tokenizer_source.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_source.token_to_id('[EOS]')\n",
    "\n",
    "    # precompute the encoder output and reuse it\n",
    "    encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "\n",
    "    # initizalize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(encoder_input).to(device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.shape[1] == max_length:\n",
    "            break\n",
    "        decoder_mask = causal_mask(decoder_input.shape[1]).type_as(encoder_mask).to(device)\n",
    "\n",
    "        # output of the decoder\n",
    "        decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
    "\n",
    "        # get the next token. Project the last token\n",
    "        probabilities = torch.softmax(model.project(decoder_output[:,-1]) , dim=-1)\n",
    "        _ , next_word = torch.max(probabilities, dim=1) \n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(encoder_input).fill_(next_word.item())])\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "    \n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_visualization_during_training(model, max_length, print_msg):\n",
    "    model.eval()\n",
    "\n",
    "    # source_texts = []\n",
    "    # expected_texts = []\n",
    "    # predicted_texts = []\n",
    "\n",
    "    # size of the control window \n",
    "    console_width = 80\n",
    "\n",
    "    batch = get_data('val')\n",
    "\n",
    "    encoder_input = batch['encoder_input'].to(device) # (batch, context_length)\n",
    "    encoder_mask = batch['encoder_mask'].to(device) # (batch, 1, 1, context_length)\n",
    "    source_text = batch['source_text'][0]\n",
    "    target_text = batch['target_text'][0]\n",
    "    \n",
    "    print(encoder_input.shape, encoder_mask.shape) \n",
    "    out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_source, max_length)\n",
    "    print(out) \n",
    "    predicted_text = tokenizer_target.decode(out.detach().cpu().numpy())\n",
    "    \n",
    "    # source_texts.append(source_text)\n",
    "    # expected_texts.append(target_text)\n",
    "    # predicted_texts.append(predicted_text)\n",
    "\n",
    "    print_msg('-'*console_width)\n",
    "    print_msg(f\"[source]: {source_text}\")\n",
    "    print_msg(f\"[expected]: {target_text}\")\n",
    "    print_msg(f\"[prediced]: {predicted_text}\")\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250]) torch.Size([1, 1, 1, 250])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference_visualization_during_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/ai/deeplearning/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 198\u001b[0m, in \u001b[0;36minference_visualization_during_training\u001b[0;34m(model, max_length, print_msg)\u001b[0m\n\u001b[1;32m    195\u001b[0m target_text \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder_input\u001b[38;5;241m.\u001b[39mshape, encoder_mask\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m--> 198\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(out) \n\u001b[1;32m    200\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m tokenizer_target\u001b[38;5;241m.\u001b[39mdecode(out\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[41], line 171\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, encoder_input, encoder_mask, tokenizer_source, max_length)\u001b[0m\n\u001b[1;32m    169\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(model\u001b[38;5;241m.\u001b[39mproject(decoder_output[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) , dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    170\u001b[0m _ , next_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m--> 171\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([decoder_input, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(encoder_input)\u001b[38;5;241m.\u001b[39mfill_(\u001b[43mnext_word\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_word \u001b[38;5;241m==\u001b[39m eos_idx:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "inference_visualization_during_training(model, max_length=context_length, print_msg=print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;11m[train]\u001b[0m: 100%|██████████| 5/5 [00:02<00:00,  1.93it/s]\n",
      "\u001b[38;5;11m[ val ]\u001b[0m: 100%|██████████| 5/5 [00:00<00:00,  5.74it/s]\n",
      "\u001b[38;5;1m[step 0]\u001b[0m: \u001b[38;5;2mtrain loss: 8.3540 \u001b[0m\u001b[38;5;2mval loss: 8.3331 \u001b[0m\n",
      "Iteration 0: Train Loss of the batch 8.3783, Time 10657.90ms\n",
      "Saving checkpoint\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# visualize some sentences during training\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iter_num \u001b[38;5;241m%\u001b[39m eval_interval:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43minference_visualization_during_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_msg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# update the iterations and if terminate if necessary\u001b[39;00m\n\u001b[1;32m     66\u001b[0m iter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/dev/ai/deeplearning/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 199\u001b[0m, in \u001b[0;36minference_visualization_during_training\u001b[0;34m(model, max_length, print_msg)\u001b[0m\n\u001b[1;32m    196\u001b[0m source_text \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    197\u001b[0m target_text \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 199\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# source_texts.append(source_text)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# expected_texts.append(target_text)\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# predicted_texts.append(predicted_text)\u001b[39;00m\n\u001b[1;32m    205\u001b[0m print_msg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39mconsole_width)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "#\n",
    "#  Train loop\n",
    "#\n",
    "# ---------------------------\n",
    "from datetime import datetime\n",
    "num_iterations = 5000\n",
    "gradient_accumulation_steps = 5\n",
    "log_interval = 10\n",
    "eval_interval = 10\n",
    "checkpoint_name = \"./checkpoints/checkpoint_\" + datetime.now().strftime(\"%y%m%d%H%M%S\") + \".pt\"\n",
    "max_iters = 600000\n",
    "\n",
    "\n",
    "batch = get_data('train') # the first batch\n",
    "model.to(device)\n",
    "iter_num = info['iter_num'][-1] if len(info['iter_num']) != 0 else 0\n",
    "for it in range(num_iterations):\n",
    "    lr = get_lr(it)\n",
    "    t0 = time.time()\n",
    "\n",
    "    # evaluate the metrics\n",
    "    if iter_num % eval_interval == 0: \n",
    "        # estimate metrics at the eval_interval and convert it to numbers if tensors. Print it. Store the info\n",
    "        out_estimates = {k: { s:(v[s].item() if isinstance(v[s], torch.Tensor) else v[s] ) for s in ['train', 'val'] }  for k,v in estimate_metrics().items()}\n",
    "        print( CFG(f\"[step {iter_num}]\", 1) + \": \" + \"\".join( [ CFG(f\"train {metric}: {out_estimates[metric]['train']:.4f} \", k + 2) + CFG(f\"val {metric}: {out_estimates[metric]['val']:.4f} \", k + 2) for k, metric in enumerate(metrics_to_eval) ]))\n",
    "        info['lr'].append(lr); info['iter_num'].append(iter_num); [info[m]['train'].append(out_estimates[m]['train']) for m in metrics_to_eval]; [info[m]['val'].append(out_estimates[m]['val']) for m in metrics_to_eval]\n",
    "\n",
    "    # do the backprop, we do it for gradient_accumulation_steps, to simulate larger batches. If gradient_accumulation_steps = 1, then it is like the usual\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True) # flush the gradients, save memory\n",
    "    \n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        fb = compute_model_metrics(batch) \n",
    "        loss = fb[\"loss\"] / gradient_accumulation_steps\n",
    "        batch = get_data(\"train\")  # get the following batch\n",
    "        loss.backward()  # do the backprop, accumulate all the gradients\n",
    "\n",
    "    # update the parameters\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # log loss of batch\n",
    "    t1 = time.time(); dt = t1 - t0; t0 = t1\n",
    "    if iter_num % log_interval == 0: print(f\"Iteration {iter_num}: Train Loss of the batch {loss.item() *  gradient_accumulation_steps:.4f}, Time {dt*1000:.2f}ms\")\n",
    "\n",
    "    if out_estimates['loss']['val'] < best_val_loss:\n",
    "        best_val_loss = out_estimates['loss']['val']\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),  # stores the weights of the model\n",
    "            \"optimizer\": optimizer.state_dict(),  # stores the optimizer state\n",
    "            \"iter_num\": iter_num,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"info\": info,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_name)\n",
    "        print(\"Saving checkpoint\")\n",
    "\n",
    "    # visualize some sentences during training\n",
    "    if iter_num % eval_interval:\n",
    "        inference_visualization_during_training(model, max_length=context_length, print_msg = print)\n",
    "\n",
    "\n",
    "    # update the iterations and if terminate if necessary\n",
    "    iter_num += 1\n",
    "    if iter_num > max_iters: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x723e1111a290>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAEoCAYAAAAT/ShaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABPCUlEQVR4nO3deVhU5dsH8O+wKirgwqoI7qjgWhpaamWhmWmaqbmnVmaLlaa0uKa4pNn25s9KcS+t1NJScS1zzyW3cBdUwFzYRAGZ5/3jbgZGFllm5gzw/VzXuZg5c2bmPjPouXmW+9EppRSIiIiILMhO6wCIiIio9GPCQURERBbHhIOIiIgsjgkHERERWRwTDiIiIrI4JhxERERkcUw4iIiIyOKYcBAREZHFMeEgIiIii2PCQURERBbnoOWbZ2ZmYuLEiVi6dCni4uLg6+uLwYMH44MPPoBOp8v1Odu3b8ejjz6aY39sbCy8vb3v+556vR5XrlxBpUqV8nwPIiIiykkpheTkZPj6+sLOrnBtFpomHDNmzMBXX32FRYsWoXHjxjhw4ACGDBkCNzc3vPHGG/k+NyoqCq6ursb7np6eBXrPK1euwM/Pr1hxExERlWUxMTGoUaNGoZ6jacKxa9cudOvWDV26dAEABAQEYMWKFdi3b999n+vp6Ql3d/dCv2elSpUAyIeVPWEhIiKi/CUlJcHPz894LS0MTROONm3aYP78+Th16hTq16+PI0eOYOfOnZgzZ859n9usWTOkpaUhKCgIEydORNu2bXM9Li0tDWlpacb7ycnJAABXV1cmHEREREVQlCEJmiYc48aNQ1JSEgIDA2Fvb4/MzExMnToV/fr1y/M5Pj4+mDdvHh544AGkpaXhm2++QYcOHbB37160aNEix/Hh4eGYNGmSJU+DiIiI7kOnlFJavfl3332HMWPGYNasWWjcuDEOHz6MUaNGYc6cORg0aFCBX6d9+/aoWbMmlixZkuOxe1s4DM1BiYmJbOEgIiIqhKSkJLi5uRXpGqppC8eYMWMwbtw49OnTBwAQHByMixcvIjw8vFAJR6tWrbBz585cH3N2doazs7NZ4iUiIqKi0bQOR2pqao5pNfb29tDr9YV6ncOHD8PHx8ecoREREZEZadrC0bVrV0ydOhU1a9ZE48aNcejQIcyZMwcvvvii8ZiwsDBcvnwZixcvBgDMnTsXtWrVQuPGjXHnzh1888032Lp1KzZt2qTVaRAREdF9aJpwfP755/jwww/x6quv4urVq/D19cXLL7+M8ePHG4+JjY1FdHS08X56ejreeecdXL58GS4uLmjSpAk2b96cazEwIiIisg2aDhrVQnEGvNxPejrg5GTWlyQiIrIZxbmGci0VM9i4EWjSBBgxQutIiIiIbBMTDjP46ivg6FFg5Upp5SAiIiJTTDjMwN9ffqakAJs3axsLERGRLWLCYQZDhmTd/m8yDREREWXDhMMMmjUD6tSR22vXAnfuaBoOERGRzWHCYSavvCI/79wBNmzQNhYiIiJbw4TDTAYMAAyL5339tbaxEBER2RomHGbi5QU8/LDcrlBB21iIiIhsDRMOM3rjDfm5axeQmaltLERERLaECYcZde0KVK4MXL4MbN2qdTRERES2gwmHGTk7A336yO233waSkrSNh4iIyFYw4TCzgQPl57FjwPffaxsLERGRrWDCYWatWwNVq8rtL77QNhYiIiJbwYTDzHQ6oH9/uX30KHDzprbxEBER2QImHBYwerT8VAqYP1/bWIiIiGwBEw4LqFEjq9Q5Ew4iIiImHBYzYoT8PHcOiI/XNhYiIiKtMeGwkBEjALv/Pt0VK7SNhYiISGtMOCzExQXo2VNuHz2qbSxERERaY8JhQa+9Jj9XrQJSU7WNhYiISEtMOCzo4YeBWrWA5GRg8WKtoyEiItKOpglHZmYmPvzwQ9SqVQvly5dHnTp1MGXKFCil8n3e9u3b0aJFCzg7O6Nu3bqIiIiwTsCFZGcHPPqo3B43TttYiIiItKRpwjFjxgx89dVX+OKLL3Dy5EnMmDEDM2fOxOeff57nc86fP48uXbrg0UcfxeHDhzFq1CgMGzYMGzdutGLkBTdsmPxMTAT27NE2FiIiIq3o1P2aEyzo6aefhpeXF7799lvjvp49e6J8+fJYunRprs8ZO3Ys1q9fj2PHjhn39enTBwkJCdiwYcN93zMpKQlubm5ITEyEq6tr8U+iANzdJeHo1An47TervCUREZHZFecaqmkLR5s2bbBlyxacOnUKAHDkyBHs3LkTnTt3zvM5u3fvRseOHU32hYaGYvfu3bken5aWhqSkJJPN2p55Rn5u2ybVR4mIiMoaTROOcePGoU+fPggMDISjoyOaN2+OUaNGoV+/fnk+Jy4uDl5eXib7vLy8kJSUhNu3b+c4Pjw8HG5ubsbNz8/P7OdxPxMnys+0NODHH63+9kRERJrTNOFYuXIlli1bhuXLl+PgwYNYtGgRPv74YyxatMhs7xEWFobExETjFhMTY7bXLqjatQFvb7k9a5bV356IiEhzDlq++ZgxY4ytHAAQHByMixcvIjw8HIMGDcr1Od7e3oi/p1Z4fHw8XF1dUb58+RzHOzs7w9nZ2fzBF9ILLwBz5gB//SUtHTYQEhERkdVo2sKRmpoKOzvTEOzt7aHX6/N8TkhICLZs2WKyLzIyEiEhIRaJ0VzCwqT6aGYm8PPPWkdDRERkXZomHF27dsXUqVOxfv16XLhwAatXr8acOXPw7LPPGo8JCwvDwIEDjfdfeeUVnDt3Du+++y7++ecf/N///R9WrlyJt956S4tTKLBq1YA335TbLAJGRERljabTYpOTk/Hhhx9i9erVuHr1Knx9fdG3b1+MHz8eTk5OAIDBgwfjwoUL2L59u/F527dvx1tvvYUTJ06gRo0a+PDDDzF48OACvacW02INoqKAwEDA3h64fBm4Z+wrERGRTSvONVTThEMLWiYcABAUBBw/DowezQGkRERUspTYOhxlkcN/w3TNOBGHiIjI5jHhsLLXX5ef//4LHDqkbSxERETWwoTDyvr0kTEcADBzpraxEBERWQsTDiurUAFo00Zur10LZGRoGw8REZE1MOHQgKFb5fZt4NdftY2FiIjIGphwaKBrV+C/Wb/45BNtYyEiIrIGJhwaKFcOeOIJub1zJ3D9urbxEBERWRoTDo0sXQo0bSqlzr/7TutoiIiILIsJh0bc3QFDcVTW5CAiotKOCYeGXnhBpsju3w+cPKl1NERERJbDhENDN29m1eRYsEDbWIiIiCyJCYeG6tSRAaSAJByZmdrGQ0REZClMODTk4CCVRwHgxg1gyxZt4yEiIrIUJhwae+GFrNvsViEiotKKCYfGHn4Y8PCQ2z/9BCQmahsPERGRJTDh0Ji9PdC3r9zOyABWrdI2HiIiIktgwmEDDOM4AHarEBFR6cSEwwY89BDw9tuATgfs3g2cPat1RERERObFhMMG6HTA7NlZ66ssXqxtPERERObGhMOGGEqdL14M6PWahkJERGRWTDhsiKcn4OgIXLgA/PGH1tEQERGZj6YJR0BAAHQ6XY5t5MiRuR4fERGR49hyhlKdpcD27TJTBQAiIrSMhIiIyLw0TTj279+P2NhY4xYZGQkA6NWrV57PcXV1NXnOxYsXrRWuxfXunXV71Srg1i3tYiEiIjInBy3f3MNQ8eo/06dPR506ddC+ffs8n6PT6eDt7W3p0DQRFAQ0agScOCHJxk8/AQMGaB0VERFR8dnMGI709HQsXboUL774InQ6XZ7HpaSkwN/fH35+fujWrRuOHz+e7+umpaUhKSnJZLNl2Vs5Fi3SLg4iIiJzspmEY82aNUhISMBgw1SNXDRo0AALFizA2rVrsXTpUuj1erRp0waXLl3K8znh4eFwc3Mzbn5+fhaI3nyyJxxbtgAxMdrFQkREZC46pZTSOggACA0NhZOTE3755ZcCPycjIwMNGzZE3759MWXKlFyPSUtLQ1pamvF+UlIS/Pz8kJiYCFdX12LHbQnNmwOHD8vtqVOB997TNBwiIiIAcg11c3Mr0jXUJlo4Ll68iM2bN2PYsGGFep6joyOaN2+OM2fO5HmMs7MzXF1dTTZb17s3YBimsmgRYBspIRERUdHZRMKxcOFCeHp6okuXLoV6XmZmJo4ePQofHx8LRaaNd94BoqIAFxfg1Clg716tIyIiIioezRMOvV6PhQsXYtCgQXBwMJ00M3DgQISFhRnvT548GZs2bcK5c+dw8OBB9O/fHxcvXix0y4itc3QEXF2Bnj3lPmtyEBFRSad5wrF582ZER0fjxRdfzPFYdHQ0YmNjjfdv3ryJ4cOHo2HDhnjqqaeQlJSEXbt2oVGjRtYM2WoMy9Z//z1w5462sRARERWHzQwatZbiDHixpsREwN9ffgKSdDz/vLYxERFR2VbiB41STm5uUgjMgDU5iIioJGPCYcP69Mm6vXEjEBenXSxERETFwYTDhj33HGD33zeUmQksW6ZtPEREREXFhMOGeXsD2ZeViYhgTQ4iIiqZmHDYOEO3ik4HHDsGrFunbTxERERFwYTDxvXoAdjbZ7VsPPccsHq1tjEREREVFhMOG1etmnSlHD0qhcDS04FevYClS7WOjIiIqOCYcJQA/fvLFNnvvgMGDZIBpAMHAvPmaR0ZERFRwTDhKEFSUmRMx8iR0sUyYgTw8cdaR0VERHR/TDhKCKWAl14COneWtVZGj5b9Y8YAEyZw9goREdk2JhwlRGYmYFgUd+5cYMsW4K235P7kybLCLJMOIiKyVUw4SggHB+DTT4GffwaqVgUOHQLmzwdeeEEe/+QTaQHJzNQ2TiIiotww4ShhunYFjhwBHn0UuHULWL4caN1a6nR88w0wYACQkaF1lERERKaYcJRA1asDkZHAtGlSo+PyZeDrr6UVZMUKmTbL5eyJiMiWOGgdABWNvT0QFiYtHZmZQNu2Ugq9Rw9g7VppCVmzBqhQQetIiYiI2MJR4j30kCQbANClCzBqlCz4tnkz8OSTQEKCltEREREJJhylSGqqVCXV62VMx65dwGOPAdeuaR0ZERGVdUw4ShEXF2DHDqBp06wpsocOAY88Aly5om1sRERUtjHhKGUCA4E9e4A33sja988/MpPlwgXNwiIiojKOCUcpVK5cVs2OypVl36VLQEiIJB9ERETWxoSjFOvaVVaZbdNGioXFxQHt2kkdDyIiImvSNOEICAiATqfLsY0cOTLP56xatQqBgYEoV64cgoOD8euvv1ox4pKnenXg998l8WjeHPj3XxnT8e23WkdGRERliaYJx/79+xEbG2vcIiMjAQC9evXK9fhdu3ahb9++GDp0KA4dOoTu3buje/fuOHbsmDXDLnHs7WUdlq1bZRptcjIwbBgwfLjMaCEiIrI0nVK2s+TXqFGjsG7dOpw+fRo6nS7H471798atW7ewbt06476HHnoIzZo1w7x58wr0HklJSXBzc0NiYiJcXV3NFntJER8PNGkCXL0q95s2BX79FfD11TYuIiKyfcW5htrMGI709HQsXboUL774Yq7JBgDs3r0bHTt2NNkXGhqK3bt35/m6aWlpSEpKMtnKMi8vma3SrJncP3JEZrasX69lVEREVNrZTMKxZs0aJCQkYPDgwXkeExcXBy8vL5N9Xl5eiIuLy/M54eHhcHNzM25+fn7mCrnEKl8e2LcPeOopuZ+cDDz9tFQpTUvTNDQiIiqlbCbh+Pbbb9G5c2f4mrltPywsDImJicYtJibGrK9fUjk6yrTZ7Pnd6tVSoZSIiMjcbCLhuHjxIjZv3oxhw4ble5y3tzfi4+NN9sXHx8Pb2zvP5zg7O8PV1dVkI2FvDyxYALz1ltyPjgZmz9Y2JiIiKp1sIuFYuHAhPD090aVLl3yPCwkJwZYtW0z2RUZGIiQkxJLhlWo6nSQZ48fL/ffeAz75BDh4MKs8OhERUXFpnnDo9XosXLgQgwYNgoODg8ljAwcORFhYmPH+m2++iQ0bNmD27Nn4559/MHHiRBw4cACvvfaatcMuVXQ6YNIk4KOP5P7bbwMtWwKffaZtXEREVHoUKeGIiYnBpUuXjPf37duHUaNGYf78+YV+rc2bNyM6Ohovvvhijseio6MRGxtrvN+mTRssX74c8+fPR9OmTfHDDz9gzZo1CAoKKspp0D3eey+rewWQxOO337SLh4iISo8i1eF45JFH8NJLL2HAgAGIi4tDgwYN0LhxY5w+fRqvv/46xhva521QWa/DcT96PTBwILBsmdx3cZEZLY0baxsXERFpz+p1OI4dO4ZWrVoBAFauXImgoCDs2rULy5YtQ0RERFFekmyEnR2wcCHQqZPcT00FQkOlJDoREVFRFSnhyMjIgLOzMwDpEnnmmWcAAIGBgSZdIFQyOToCP/4IPPig3L98WWp2sEYHEREVVZESjsaNG2PevHn4448/EBkZiU7//Tl85coVVK1a1awBkjZcXICNG4F69eT+gQMAG6+IiKioipRwzJgxA//73//QoUMH9O3bF02bNgUA/Pzzz8auFir5KlcGtm0DPD3l/vz5QBmvDE9EREVU5MXbMjMzkZSUhMqVKxv3XbhwAS4uLvA0XKFsEAeNFt6pU8DDD8s4jkcflcXeypXTOioiIrI2qw8avX37NtLS0ozJxsWLFzF37lxERUXZdLJBRVO/vkyPrVRJWjwCAqSLhYiIqKCKlHB069YNixcvBgAkJCSgdevWmD17Nrp3746vvvrKrAGSbWjZElizRmaxxMcDHToAV65oHRUREZUURUo4Dh48iEceeQQA8MMPP8DLywsXL17E4sWL8RnLU5Zajz0mU2YB4NYtmcVy+7a2MRERUclQpIQjNTUVlSpVAgBs2rQJPXr0gJ2dHR566CFcvHjRrAGSbRk4EJgyRW5fuQK0bcs1V4iI6P6KlHDUrVsXa9asQUxMDDZu3Ignn3wSAHD16lUOxCwDPvgAMFSiP3QIeO45beMhIiLbV6SEY/z48Rg9ejQCAgLQqlUr42qtmzZtQvPmzc0aINmmb74BHn9cbv/0E5BtjT0iIqIcijwtNi4uDrGxsWjatCns7CRv2bdvH1xdXREYGGjWIM2J02LNR6+XNVb++QdwdpYZLP/lnkREVAoV5xpa5ITDwLBqbI0aNYrzMlbDhMO87twBunYFNm+WQmF//MGF3oiISiur1+HQ6/WYPHky3Nzc4O/vD39/f7i7u2PKlCnQ6/VFeUkqocqVk+myISHAzZsyXfbkSa2jIiIiW+NQlCe9//77+PbbbzF9+nS0bdsWALBz505MnDgRd+7cwdSpU80aJNm2ChWAdeuA4GCZufLgg8DZs4CXl9aRERGRrShSl4qvry/mzZtnXCXWYO3atXj11Vdx+fJlswVobuxSsZw1a4Bnn5Xb3t5SEv2/2dNERFQKWL1L5caNG7kODA0MDMSNGzeK8pJUCnTvDsycKbfj4oCHHuKS9kREJIqUcDRt2hRffPFFjv1ffPEFmjRpUuygqOQaMwYYMkRunzgBdOoEZGZqGxMREWmvSF0qO3bsQJcuXVCzZk1jDY7du3cjJiYGv/76q7HsuS1il4rl6fVA+/bAzp1yv29fYNkyQKfTNi4iIioeq3eptG/fHqdOncKzzz6LhIQEJCQkoEePHjh+/DiWLFlSlJekUsTODtiwAahVS+6vWAFMmKBtTEREpK1i1+HI7siRI2jRogUybbgNnS0c1nP5MvDWW8CqVXL/s8+A11/XNiYiIio6q7dwmNPly5fRv39/VK1aFeXLl0dwcDAOHDiQ5/Hbt2+HTqfLscXFxVkxaiqI6tWBlSuByZPl/htvSGsHERGVPUWqw2EuN2/eRNu2bfHoo4/it99+g4eHB06fPo3KlSvf97lRUVEm2ZWnp6clQ6Vi+OADae343/+A/v2lImmnTlpHRURE1qRpwjFjxgz4+flh4cKFxn21DB3/9+Hp6Ql3d/f7HpeWloa0bHMzk5KSCh0nFY9OBxgmL+n1wNNPAxMnAmPHAo6OmoZGRERWUqiEo0ePHvk+npCQUKg3//nnnxEaGopevXphx44dqF69Ol599VUMHz78vs9t1qwZ0tLSEBQUhIkTJxornt4rPDwckyZNKlRcZH4jRgC//w58/71Mk/3wQ2D1amDhwqxkhIiISq9CDRodYiiwcB/ZWyzyU65cOQDA22+/jV69emH//v148803MW/ePAwaNCjX50RFRWH79u144IEHkJaWhm+++QZLlizB3r170aJFixzH59bC4efnx0GjGrh9G3jiCeDPP6XVQylp4fjwQ2DcOLZ2EBHZOk1Xiy0OJycnPPDAA9i1a5dx3xtvvIH9+/dj9+7dBX6d9u3bo2bNmgWakstZKtpKSgJCQ4E9eyTByMiQ/c2bAxERbO0gIrJlJXaWio+PDxo1amSyr2HDhoiOji7U67Rq1QpnzpwxZ2hkIa6uUqOjVStJNry8ZBDpoUPAAw8AU6ZkJSFERFR6aJpwtG3bFlFRUSb7Tp06BX9//0K9zuHDh+Hj42PO0MiC3NyAjRulpWP9eimB3r27JBrjxwOtWwNHjmgdJRERmZOmCcdbb72FPXv2YNq0aThz5gyWL1+O+fPnY+TIkcZjwsLCMHDgQOP9uXPnYu3atThz5gyOHTuGUaNGYevWrSbPIdvn7i4tHS1bysqyP/0ELFkCVKmS1doxeTJbO4iISgtNE44HH3wQq1evxooVKxAUFIQpU6Zg7ty56Nevn/GY2NhYky6W9PR0vPPOOwgODkb79u1x5MgRbN68GY8//rgWp0BmsnevdKesWSOtHXfvSjn0Vq3Y2kFEVBpoOmhUCxw0anuUAjp0kGmz3t7Atm3SyvHaa8CNG4CDg8xkCQvjTBYiIi2V2EGjRIBMkf3xRyA4GIiLAx57TLpaTpwAnn2WrR1ERKUBEw6yCdWqAVu2AEFBQGws8OijQHKyJCIrVgBVqwKHD8vYjkmTgPR0rSMmIqLCYMJBNsPDQ5KORo2AK1ck6Th3DujTBzh+PKu1Y+JEae04fFjriImIqKCYcJBN8fQEtm4FGjYELl0Cpk6V/V5epq0dR44ADz4oyQdbO4iIbB8TDrI5Xl6SdIwcCfzf/2Xt1+myWjt69JDWjkmT2NpBRFQSMOEgm+TtDXzxBfDfcjtQSmasAJKQ/PAD8N13bO0gIiopmHCQzVMKePddGTBqKMmi0wG9e7O1g4iopGDCQTYvIUGWsj9/XgaSXrqU9VherR0vvSRjPq5f1yxsIiLKhoW/qESIiZHiYOfOAXXqADt2ANWrmx5z9Srw6quSaBjodECzZlLb47HHgEceASpVsmbkRESlR4ldnl4LTDhKruhoSTrOnwfq1QO2bwd8fU2PUQrYvBn45RcZeHr8uOnjDg7S7WJIQEJCssaJEBFR/phwFAITjpLt4kWgfXv5Wb++JB35LRQcFyel0rdule3cOdPHnZ2Btm0l+Xj8cRkn4uBg0VMgIiqxmHAUAhOOku/8eWnpiIkBVq4Ennuu4M+9cCErAdmyRaqaZlepEtCuXVYLSJMmgB1HOhERAWDCUShMOEqHc+eA/ftlpkpRKQVERWW1fmzbljX11qBqVRmoakhA6teXcSFERGURE45CYMJROsXHSyLg6Vn019Drgb//lpaPrVtl9dqUFNNjfH2zul8efxzw8yte3EREJQkTjkJgwlH6xMZKEuDoKMmCh4d5XjcjAzhwIKsF5M8/gbQ002MaNACeeEK2Dh0A/koRUWnGhKMQmHCUPqdPy0DS2FgZc7Fli6w+a263bwO7d2eN/9i3T1pFDBwcgIceykpAHnyQA1CJqHRhwlEITDhKp6goaWGIi5O6G1u2AFWqWPY9ExJk3EdkpGxnzpg+7uYm4z+efFISkDp1OP6DiEo2JhyFwISj9Dp5Ui7w8fEyuPPLL4GOHa33/ufPZyUfW7YAN2+aPh4QkNX68fjjlk+IiIjMjQlHITDhKN1OnJAkwzDdNSICGDTI+nFkZgIHD0rysWkTsGuXjAkx0OmAli2zWj9CQqQmCBGRLWPCUQhMOEq/hARgwgRZf+X4cdsoZZ6SIrNeDC0g91ZAdXGRcSiGFpDGjdn9QkS2hwlHITDhKDtu3QIqVJDbSgGDBwN9+gCdO2saFgDg8mUpwR4ZKT/j400f9/GRxGPwYBmbwuSDiGxBca6hmtdQvHz5Mvr374+qVauifPnyCA4OxoEDB/J9zvbt29GiRQs4Ozujbt26iIiIsE6wVKIYkg0AWLECWLwYeOopoHt3qTiqperVpatn6VLp/jlyBPj4Y+liKVdO9i1eLNN9H3pIWmuyz4ghIippNE04bt68ibZt28LR0RG//fYbTpw4gdmzZ6Ny5cp5Puf8+fPo0qULHn30URw+fBijRo3CsGHDsHHjRitGTiXN008Db78N2NsDa9cCDRsCkycDd+5oHZm0XjRpArzzDrBxoww23bwZGDFCko99+4AePYBGjYCFC4H0dK0jJiIqPE27VMaNG4c///wTf/zxR4GfM3bsWKxfvx7Hjh0z7uvTpw8SEhKwYcOGHMenpaUhLVu1pqSkJPj5+bFLpYw6fhx47TVZ9A0AatcGPv1UEhJbdPUq8NlnMuMmIUH2Va8uydPw4bYxPoWIyo4S26Xy888/44EHHkCvXr3g6emJ5s2b4+uvv873Obt370bHe+Y6hoaGYvfu3bkeHx4eDjc3N+Pmx1rUZVrjxlK4a8UKKVN+7hzwwQcyq8QWeXoCH30kq+POmiVjOy5fltYQf39g/Hjg33+1jpKI6P40TTjOnTuHr776CvXq1cPGjRsxYsQIvPHGG1i0aFGez4mLi4OXl5fJPi8vLyQlJeH27ds5jg8LC0NiYqJxi4mJMft5UMmi08ng0X/+AcaMkdYDe3t5LC0NSE3VNr7cuLoCo0dLrY+vvwbq1ZOulylTJPF44w1JSoiIbJWmCYder0eLFi0wbdo0NG/eHC+99BKGDx+OefPmme09nJ2d4erqarIRAdIdMXMm0LZt1r6ZM2WsxJo1MrPF1jg7A8OGSZGzH36QWh63bwOffy6VTAcMALL1NhIR2QxNEw4fHx80atTIZF/Dhg0RHR2d53O8vb0Rf88cwvj4eLi6uqJ8+fIWiZPKhowMmTVy8SLw7LMyo+X0aa2jyp29PdCzJ7B/vwww7dhRuoWWLgWCg4GuXWWxOS1lZsrnt2+f7XZZEZH1aJpwtG3bFlFRUSb7Tp06BX9//zyfExISgi1btpjsi4yMREhIiEVipLLD0VGqg4aFye0NG4CgIOD996Wmhy3S6aRMemSkJB/PPSf71q0DHn4YeOQRYP36vFtrUlKKH4NSsoZNZCQwZw4wZIgsXFepkpSYb91ayrpPmMBuH6IyTWlo3759ysHBQU2dOlWdPn1aLVu2TLm4uKilS5cajxk3bpwaMGCA8f65c+eUi4uLGjNmjDp58qT68ssvlb29vdqwYUOB3jMxMVEBUImJiWY/Hyo9oqKU6tRJKbmcKuXnp9Tvv2sdVcFERSk1bJhSTk5Z8QcHK7V0qVIZGXLMhQtKdekij82dW/DXTk5WavdupebPV+qNN5R69FGlqlXLep97t3LllHJ1zbqv08nn+sMPSqWnW+b8ichyinMN1TThUEqpX375RQUFBSlnZ2cVGBio5s+fb/L4oEGDVPv27U32bdu2TTVr1kw5OTmp2rVrq4ULFxb4/ZhwUEHp9UqtXq2Uv79Szs5KnTmjdUSFc/myUmPGKFWxYtYFPyBAqcGDlXJxydrn4KDU3r2mz01PV+rYMaW++06p999X6plnlKpVK+/EQqdTql49pXr0UGrCBEko/vlHqbt3lbp9W6nly5V67DHT53h6KvXuu0qdOqXJx0NERVCcayhLmxPdR2oqsGePVP00WLUK6NSpZNTBuHkT+OorYO5c0ym0AQHS5bFpE+DtDbzyioy5OHpUZvDkVWDM21vGiRi2oCAZaOvicv9YzpwBvv1WCphlH4rVvr3UFenZU4qd2ZLff5cZQunp0m3k4aF1RETa4VoqhcCEg4przx5Z3dXDAxg7Fnj1VcCWxyunpMgA0nbt5EI/a1bBSrtXrCjJhCGpMCQY1aoVP6aMDBlb8vXXMlbGULa9cmWZaTNsmLyXli5fBt59F1i+PGtfaCjw66+AneaLQhBpgwlHITDhoOLasUMuiGfOyH1vbxlYOny47S0x/8svwMiR0ppw+LCUdL97F1i5Epg+XVoz7O2lk6NFC1lnxpBY+Ptb58IaEwMsWCAtH9nL5LRuLZ9p796S/FhDUhJw6BAwezbw22/yWWXn5gbs2iUtOkRlEROOQmDCQeZw964srjZ5ctbMCz8/qVo6ZIjMctHSlStSDOzHH+V+rVrAsmXSMmOgFHDpklQzTU2V1gUtZWZKl8XXXwM//5x1sa9YEXjhBUnyHnjAfCvn/vuvJBeHDsnspEOH8p4GXaUKcOOG3PbwkISkf3+u4ktlDxOOQmDCQeaUni5/nX/0kTTB+/pKy4dWXSx6PfC//wHjxslf6/b2UgZ9woSCjbEAgMREid/JybKx5ic+Hli0SJIPQ0sSADRtKq0e/foB7u4Fey1DYpU9sTh4UPblpkYNGdvi6ior+rZsKfv++EPGuZw8Kcd16ADMmwc0aFCcMyUqWZhwFAITDrKEO3fkQl+lioxBAOQv9l9+AZ55xjpdE0oBTzwBGMrUtGoFzJ8vF+mC2rMH6NsX6NVLqq5qTSnpwvr6a2mtMazDWK6cxDh8uNQbMbQ06PXA2bNZSYUhwbh2LffXr11bXuvUKfnsFi3Kf1Boejrw8ceSwN29Ky1ZYWGy2dpgVyJLYMJRCEw4yFqWLZNm98aNgUmTpHqppROPmTOltWXaNFne3rBGTEGtWSNxAsDGjcCTT5o9xCK7cUMqqX79tWn59gYNpLXhxAkZp5KcnPO59vYy7qJ5cxmr0qyZjBd5/33AUNj4iSdkQKiDQ/5xXLggr5V96aa6dWUm0D3rShKVOkw4CoEJB1nL11/LLAfDsvLNmslia126mK/vf8sWoEIF4KGH5H5GhixpX7160V/z1Vfl4unlBfz9t4zxsCVKAXv3yuf73Xc5F9tzdgaaNJHEwpBgBAVldXP984+Mb4mMlPt+flIhtWfPgn8vixYBgwfL8ZUrZ43veOEFGd/h7W2WUyWyOUw4CoEJB1lTQoJczObOzfrLu3VrGWz6xBNFTzz+/VfGZixZIjNPDh0y3wyZ27elNPnx40DnzlIm3VangSYlAd9/L10iQUGSXAQG5j1od/Vq4PnnpTvEyUkSwnHjJGkrrCFDgIgIScieflpu6/Uyk2X6dOCll2z3cyvtMjMzkZGRoXUYJZaTkxPs8vjlZcJRCEw4SAvXr0v9i88/l7/I27QBdu4sfMKhlPx1PXq0vKZOJ9Nep08v2kUzL8eOSdJx5w7wySfAqFHme20t3bghA0LbtJHzqlOn6K+VmirjZI4fl6Jw06bJd/HXX/J469YyrqcwY2ioeJRSiIuLQ4KhWZGKxM7ODrVq1YJTLiPHmXAUAhMO0lJ8PDBjhgwk7dBB9iUkyPiDNm3yf+6pUzJLYts2ud+kiQwKbd3aMrH+3//JBdTRUbowmje3zPtY0uHDMpZm5sys5O7y5eJ1OWV38qQkZrduARMnyrToL7+Un8nJMnZk1Ch5zFq1RMqy2NhYJCQkwNPTEy4uLtBx3nKh6fV6XLlyBY6OjqhZs2aOz7BY19Di1FQvibiWCtmaDz+UtUU6d1Zq//7cjzl6VNZzAZQqX16pGTMsv/iZXq9Ut26yjsq//1r2vczt+nWlXn1VKTs7+cx++MFy77VkiaxH88knWfsuXVKqV6+sdWNq1JB1echy7t69q06cOKGuXbumdSglXkJCgjpx4oRKz+U/meJcQ+8zHpuILC0lRf4S/u032bp1kzEeTZpkHdO4sUz/dHCQAZ21alk+Lp1OynqXL19yClxlZkrF0vfeky4nQCqVtmpluffs318KqmXvnqleXaq5/vqrtBJduCCzf555RrrVata0XDylUXq6jFPatQvYvVtaBOvUkdalVq2kIJyLi4zZcClowRnKk6ErJTMzE45mrGLILhUiG3DmjCQZy5ZlrSvi7y9dAoYCV8nJ0iyv5cU/Otp2L5Z79gCvvZY1hiIoSC7uhq4ra0lJkSTNMCU5NVWmKs+aJYNVXVxkmvSbb2pfkdZWxcZKYmFIMP76K6sGS17atbuDjz46j1q1asHdvZzJd0CFc+fOHZw/L59luXsKzHAMRyEw4SBbdvKkXIy+/17uv/KKtGho7dYt4OWXpZDZ4cPWaWEpDL1eEoyTJ2WWyOTJMr33fjU1zO3oUSlI1ru3fI/ZHT8utVH++EPuN2kilUqzl5svizIygCNHJLEwJBmG5QKyq1pVxjmFhMhnd/o0sH+/bKdPA/7+dzBv3nlUq1YLgFwky5eXwdSGrVw5zhwqCCYcZsKEg0qCv/+WKa9du8oqr1q7e1fi2L1b/sP//XfrX8xzo1RWi8+mTZKohYdrVztk+XIpu67TSeG0J54wfVyvl+mzY8bIjBmdTqbPhodrv5aNtfz7r2nrxf79pkXUAEkKgoLkd82QZNStm3fr3s2bwMGDd1ChwnlUrlwLd+6UQ26zYnU6aWHKnoQ4O1um1VAp6eK7ezfnlpEhP+3spGaLuRd9DAgIwKhRozCqiNPLmHCYCRMOoqK5cEGmeCYlySyMKVO0iyU5WWZ/tGwpLRm25OWXZfaQh4e0Bvn65jzm2jVJOiIi5L6np9RreeGF+1/8lJLEJTMz6+e9t/O77+gof/kbNmdny/3Vf/euTLHO3npx9mzO4ypXluJ1hgSjVSugUqXCvde9F8n0dGmZu3VLurVu3ZLzv5e9vSQe2ROR3NYR0utzTx7uTSKyb/e7uj74YP5f9oQJEzBx4sSCfwj/+ffff1GhQoUij2dhwmEmTDiIiu7774E+feSiuHWr9cdHAHLRGjAAOHdOLg4XL0pzu624fVsunEeOAO3bA5s3590atGOHdLMYFoSrWlUu/vklDYYxPubk7GyahBi2cuUKt798eYn/778lwdi3T8a03KtRo6yWi5AQKU9f3KQnv4skIBf/tLSsJMSQiOR2BTQkZdlbKHJLVgrCzk6+f0dH+Zl9O3MmDrduyXE7d36Pzz8fj6ioKONzK1asiIr/zadWSiEzMxMOVmhatFTCYQONokRUUvTuLV0XCxbI7IwjR6x3sc/IkFaVqVPloluzpnQ72VKyAciFauVKaX3ZsUNqcHz0Ue7Htm8vrSAffyznZphZU1w6nfzlbm8vF7zstzMyJCnKfgFNS5PNEvWyKlXKar0ICZG6MUXtPkpMlAu34Q/36GhpHcpt4Tylcpa9B+TYcuXk90avl+J22VtB7tyRzycpKfcYHBzks7w3iTDsc3CQcUSGx/JLpLy9vREfL7Vh7O3doNfr4OjojSpVgB07tuPRRx/Fr7/+ig8++ABHjx7Fpk2b4Ofnh7fffht79uzBrVu30LBhQ4SHh6NjtoV8AgIC8Oabo/DWW6MAADqdDl9//TXWr1+PjRs3onr16pg9ezaeeeaZAn7y5sGEg4gK5bPPgD//BKKiZLXWn36y/HueOiUJzv79cr9/f+CLL+Q/dltUv76s9dK3r1Qgbdcu74XwnJxkGu/LL8uF594EwXA7t+Qhr9sFGZNgSDwMF9jsW277CnpsWhpQr15WC0ajRuaZLaIUMGiQtGytWiUJ51NPyWt//z0QEGB6fGqqdsXWUlJy75a5l04nYzjc3GTMDwCcPy9jUu7elfvjxo3Dxx9/jNq1a6Ny5cqIiYnBU089halTp8LZ2RmLFy9G165dERUVhZr/TSFTSmb6JCRkzXKbNGkSZs6ciVmzZuHzzz9Hv379cPHiRVSpUsXs558XJhxEVCgVKgArVgA9elhn/MTNm1JvISlJ/vOcN09aWmxdnz7SwnHqlGlNlbxUrWrd1hpHR9lKSs/yJ58Aa9fKhfzWLflcr16VQagtWwLffCMLJJZE5csDPj5ZyWJCgoyZAoDJkyfjiWyjj6tUqYKm2erlT5kyBatXr8bPP/+M1157zaQbKC4uKykfPHgw+vbtCwCYNm0aPvvsM+zbtw+dOnWy0lkCmk4QmjhxInQ6nckWGBiY5/ERERE5js+tr46ILKt5c/kP3xrLsVeuLAvVPfaYTDstCcmGwdy50gXF1WOLZ88eYOxYuf3JJ7JIX9Om0qX32GPSmhEWJoNxDV1FLi7S0qDFVpSxmjqdbA0bZo0fAQAPjweMrR0AkJKSgtGjR6Nhw4Zwd3dHxYoVcfLkSURHR0MpSVSUkpafOnWyWruaZMt6K1SoAFdXV1y9erXwgRaD5i0cjRs3xubNm4337zcgxtXV1WRQDWvlE2kje9Gq8+dlOXtzFXlct06ayIOC5P7778vMmJJWQ+He6Y5HjwLBwdrEUlJdv561wm/v3jLI1sDHRxK66dNlXNGtWzILpk6drBknJY2LiyQdJ07I/bS0Cjh+XAoBursDo0ePRmRkJD7++GPUrVsX5cuXx3PPPYf09HRcvSotggBQrZrpv9F7K4bqdDroLTECOR+a//N1cHCAt7e3catWrVq+x+t0OpPjvby8rBQpEeXmxx/lr8133in+a926JcXOunaVehaG6pKGsQkllV4vF8omTaR8PRWMXi/jNmJiZFzI/Pk5x6fY20tCumSJ3E5Pl7EwJZmdnUyrBiRpzciQasTnzwM7d/6JwYMH49lnn0VwcDC8vb1x4cIFpKfL5wTI52Du2h7moPk/4dOnT8PX1xe1a9dGv379EB0dne/xKSkp8Pf3h5+fH7p164bjx4/ne3xaWhqSkpJMNiIyH1dXqYsxbx6wenXRX2ffPumq+d//5L41umusxTCoE5ApvZcuaRtPSfHZZ8D69TKrZNWq/MebtGghNU/c3XMOIC3JAgOl9RCQ1h4vr3pYteonHD58GEeOHMELL7wAvV6PxEQ5pkoV2y3prmnC0bp1a0RERGDDhg346quvcP78eTzyyCNITk7O9fgGDRpgwYIFWLt2LZYuXQq9Xo82bdrgUj7/esPDw+Hm5mbc/Pz8LHU6RGXSE08A774rt4cOzforq6Du3pVS5G3aSInq6tWldsXs2bb5V1pRzZ4tF8Xr12VAaW6VMMlUz57ye/HZZ9KKdj92dkCNGqYzRK5ckYS4pLKzA/z8JPFwdgbefHMOnJ0rIySkDbp27YrQ0FC0aNHCWBvF31/riPNmU4W/EhIS4O/vjzlz5mDo0KH3PT4jIwMNGzZE3759MSWPsodpaWlIy7bqT1JSEvz8/Fj4i8iM0tOBtm2BAwdkCujWrQX7K+vaNek+2bNH7vfuLWvHlNYy32fPStKRlCRJ2owZWkdk++7eld+l+w3Xy61YVWKiJLGAtH74+JSclY9zk5kp3UWGsZ7OztKaY6jKmplpntYNSxX+0rxLJTt3d3fUr18fZ86cKdDxjo6OaN68eb7HOzs7w9XV1WQjIvNycpKpshUryjor06YV7HmVK0txJFdXYOlSeY3SmmwAMphxwQK5PXOmDI4lU3p91gJ3gPx+FDVJqFgxa6rxlSsysyo9vfgxasXeXuqP1K8v/+bS0qQeTkyM+ZINS7KphCMlJQVnz56Fj49PgY7PzMzE0aNHC3w8EVlO3bpZK9tOmpRVpOte165lLdZlby+Jxt9/Zy16Vtr17Am8/rrcHjQIxr53EuHh0kr2/vvFfy17e1nZuFYt6ZpITpbZH5aoqGpNrq5A7dpZ9+PjpTx+bmXkbYmmCcfo0aOxY8cOXLhwAbt27cKzzz4Le3t7Y3GSgQMHIiwszHj85MmTsWnTJpw7dw4HDx5E//79cfHiRQwbNkyrUyCibPr3BwYOlIXVcutz/+03mRaa7Z81/P1tu9/ZEmbNksqjCxfabrVULWzfDowfL7fr1TPf61atKhVPXVyki+bMmZI9k+XuXZmxAsg5OTpKxdd//pEByVae7VpgmtbhuHTpEvr27Yvr16/Dw8MDDz/8MPbs2QOP/+YDRUdHwy7bXLibN29i+PDhiIuLQ+XKldGyZUvs2rULjRo10uoUiOgeCxfmnMKamipjFr78Uu5v3iz7zFW3o6RxdgY2bCgbLToFFR8vpeD1emDIEGDwYPO+frlyMvDy8mV5r/Llzfv61qKUJBtpadKtUr++7I+JkQHJcXHSahYQYHt1SGxq0Kg1cLVYIuvJyJAE5JNP5K8vAHjjDSnUVFL/w7eEmBhZI6R9e60j0UZmprT4bN0qxd727i18Mnq/1WKzuzfZzcgwLZJly65ckU2nkwQqe1Jx86asnnz3rjzu4yNVbgtbw4arxRJRiXLjhkyZPXhQ7vv4ABEReS9iVlYdPQp06CB/uR46VPa6lwBZKXfrVrl4rlpl+Zav7K+fni7jOtzdZfqpLQ+8TEyUZAOQ35N7WzAqV5aBstHRknxcuSLjVWrVso0EnwkHEVmEu3tWtcQePaRKpK0tJW8LGjSQ2Sv798uU2aAguV+7tvzs2lW7VU+tISpK6rAAUvQtn+W0LCI5WVoErl2TQZe1a9tmV59eL60XgPy7yqsot6Oj/N7cuCGJR2qq7YzpYMJBRBZhZyfTPs+ckYsqxyvkzskJWLlS6phcuSLTin//Pevxf//NSjjmzAH+/NM0IaldW6ZKlpQugXs1aAB89510o/TrZ/33r1pVvoNz52Tg5cmT0tLh4WFbv7N2djKQNjZW4rufKlWkPkdSku2M5WDCQUQW4+Bg/b9YS6KAAClQdfSoXPjOnpWfV66Ytgpt3w788kvO5xvqMxw6lDXr5e+/5S/3OnVsfybM88/LppVKlWQWy4UL0m0RHS0Xan9/20rkypc3nQ57P46OttWqyISDiMgGuLgArVvLlpfRo2WNmexJieEv83//NV1rZPx4YO1auV2lSlaLSJcuwAsvaD9WISICCA2VsT22wNFRaslcvSpTSxMS5AJfvbq2cV2/Li0whmqi9+rQoQOaNWuGuXPnWjWuomDCQURUQrRrJ1t2er1MhYyNNe0CcHUFPD3lAnrjhmwHDgDffy9dMzt3atfU/ttvMvXVyws4dizv8QjWptNJTBUrSuuSt3fWY+np0mJnzlWLu3btioyMDGzYsCHHY3/88QfatWuHFSuOoF69JjlmpJRETDiIiEowOztZJ8TX13T/4sXyMyUlqyXk4EFZCK1+fe0uXjExsmIuIFVXbSXZyK5CBdPCY0pJi1JmprR4uLubZ3zH0KFD0bNnT1y6dAk1atQweezbbxeiceMHULduE7i52eZA1sKyqdLmRERkXhUrAk2aAN27y2yQM2ekLopBdDQwYoS0klhaRoaslHv9uszImTPH8u9561be2507BTv25k0Z23HnjiQeUVFSPCy3Ywvj6aefhoeHByIiIkz2JyenYNWqVWjXrjvGj++L9u2ro0IFFwQHB2PFihXF+0A0xISDiKgMqVbNtDXk/feBefNk/MKUKYW/aBbG++8Du3ZJd8/KlVJx1dIqVsx769nT9FhPz9yPq1oVGDtWxpvY2UmrUWBg7scWhoODAwYOHIiIiAhkr8E5f/4qZGZmokuX/njkkZZYv349jh07hpdeegkDBgzAvn37zPDJWB8TDiKiMuzll4FWrSTRGD9eulsWLJDuA3P65RdZQwaQ169Tx7yvb2k6nXSnBAWZtxvoxRdfxNmzZ7Fjxw4A0pqybNlCPPZYT4SE+CMsbDSaNWuG2rVr4/XXX0enTp2wcuVK8wVgRRzDQURUhj38MLBnj7Q4hIXJOh1DhwJz50qXR8eOxX8PpaScPSCl7e9tWbCk/FZQvXemztWreR9rGCzq5JQ1jfnyZekmCgzMGtOhVOHGdwQGBqJNmzZYsGABOnTogCNHzuDQoT/w/feT4eaWiSlTpmHlypW4fPky0tPTkZaWBpcSOqCDLRxERGWcTgf07i1Fr2bPlkGRR49K3Q9zvf7GjcCECVmtHNZSoULe271LruR37L2lwatVkxWRmzeXrhTD6508KYlLYap7Dh06FD/++COSk5OxadNC1KpVBz17tsesWbPw6aefYuzYsdi2bRsOHz6M0NBQpKenF/+D0QATDiIiAiBjKt5+WwZGhoXJCr8GJ09mreNRFBUrAhMnSgtBaZK9leTaNSklHh0t67PcvCktHvfTq9fzsLOzw/Lly7FkyWIMG/Yi7O11+PPPP9GtWzf0798fTZs2Re3atXHq1CnLnYyFMeEgIiITVaoA06ZlFRJTSupm1KsnrRT5dVNkt3o18PHHBbvolgYeHlLx1cEha0bLP//c//NKTa2Izp17IywsDLGxsRg8eDAAoF69eoiMjMSuXbtw8uRJvPzyy4iPj7f8iVgIEw4iIsrXjRsyhiE1VabW1q0LfP21lE7Py7lzkqSMGSNVRcsCOzuZ6RIcnDWj5dYtSTrOnMm9myU5WWqTdOo0FDdv3kRoaCh8/5tG9MEHH6BFixYIDQ1Fhw4d4O3tje7du1v3pMxIp1RZyT1FUlIS3NzckJiYCNfsdYCJiChPSgE//ijTQ8+dk32NG8uYjE6dTAdKpqXJYnR//QW0aSNjQSy9JsmdO3dw/vx51KpVC+XuHZyhkfR06Ya6dk2Wjr93Zk56unS93L0rrUq1atnGgnH5fZbFuYayhYOIiO5LpwOee07GcnzyiVwgjx8HnnpKSpVn9847kmxUrSorwdrSAmjWZJjR0rgxkL2QaFqaJCLnzkmyUb68LBRnC8mGJTHhICKiAnNyAkaNki6Cd96RxeZCQ7MeX74c+PJLub1kScGWUi/typc3LXJ25YpsKSky6LROHe0X07MG1uEgIqJCq1xZBoRmZmZdLJOSgH795Pa4cUDnztrFZ8vc3SXZSE+XbhQb6QGyOCYcRERUZNn/Mv/hB/nZvr2USafcVa4sSUdmpsxoKSvK0KkSEZElDRkiZdLr19fuQlpS5kHodLabbFjqM9R0DMfEiROh0+lMtsDAwHyfs2rVKgQGBqJcuXIIDg7Gr7/+aqVoiYgoPzqdrDWiRXEvx/9Gpqamplr/zUsZQyVTezMPLNE8v2rcuDE2b95svO+QT8q3a9cu9O3bF+Hh4Xj66aexfPlydO/eHQcPHkRQUJA1wiUiIhtkb28Pd3d3XP1vQRQXFxfoSvu0DwvQ6/X4999/4eLiku/1uCg0TzgcHBzg7e1doGM//fRTdOrUCWPGjAEATJkyBZGRkfjiiy8wb948S4ZJREQ2znAtuZrfKmx0X3Z2dqhZs6bZEzbNE47Tp0/D19cX5cqVQ0hICMLDw1GzZs1cj929ezfefvttk32hoaFYs2ZNnq+flpaGtLQ04/2kpCSzxE1ERLZFp9PBx8cHnp6eyMjI0DqcEsvJyQl2duYfcaFpwtG6dWtERESgQYMGiI2NxaRJk/DII4/g2LFjqFSpUo7j4+Li4OXlZbLPy8sLcXFxeb5HeHg4Jk2aZPbYiYjINtnb25t9/AEVn6aDRjt37oxevXqhSZMmCA0Nxa+//oqEhASsXLnSbO8RFhaGxMRE4xYTE2O21yYiIqKC0bxLJTt3d3fUr18fZ86cyfVxb2/vHCvlxcfH5zsGxNnZGc7ZS7wRERGR1dlUafOUlBScPXsWPj4+uT4eEhKCLVu2mOyLjIxESEiINcIjIiKiItK0hWP06NHo2rUr/P39ceXKFUyYMAH29vbo27cvAGDgwIGoXr06wsPDAQBvvvkm2rdvj9mzZ6NLly747rvvcODAAcyfP7/A72koaMLBo0RERIVjuHYWpTiYpgnHpUuX0LdvX1y/fh0eHh54+OGHsWfPHnh4eAAAoqOjTUbKtmnTBsuXL8cHH3yA9957D/Xq1cOaNWsKVYMjOTkZAODHFYWIiIiKJDk5GW5uboV6jk6VlDqwZqLX63HlyhVUqlTJbHOMk5KS4Ofnh5iYGLi6uprlNUuKsnruPG+ed1nA8y5b5w3c/9yVUkhOToavr2+hp87a1KBRa7Czs0ONGjUs8tqurq5l7pfToKyeO8+7bOF5ly1l9byB/M+9sC0bBjY1aJSIiIhKJyYcREREZHFMOMzA2dkZEyZMKJP1PsrqufO8ed5lAc+7bJ03YNlzL3ODRomIiMj62MJBREREFseEg4iIiCyOCQcRERFZHBMOIiIisjgmHGbw5ZdfIiAgAOXKlUPr1q2xb98+rUMyq/DwcDz44IOoVKkSPD090b17d0RFRZkc06FDB+h0OpPtlVde0Shi85g4cWKOcwoMDDQ+fufOHYwcORJVq1ZFxYoV0bNnzxyrGZdEAQEBOc5bp9Nh5MiRAErPd/3777+ja9eu8PX1hU6nw5o1a0weV0ph/Pjx8PHxQfny5dGxY0ecPn3a5JgbN26gX79+cHV1hbu7O4YOHYqUlBQrnkXR5HfuGRkZGDt2LIKDg1GhQgX4+vpi4MCBuHLlislr5PZ7Mn36dCufSeHc7zsfPHhwjnPq1KmTyTEl8Tu/33nn9u9dp9Nh1qxZxmPM8X0z4Sim77//Hm+//TYmTJiAgwcPomnTpggNDcXVq1e1Ds1sduzYgZEjR2LPnj2IjIxERkYGnnzySdy6dcvkuOHDhyM2Nta4zZw5U6OIzadx48Ym57Rz507jY2+99RZ++eUXrFq1Cjt27MCVK1fQo0cPDaM1j/3795ucc2RkJACgV69exmNKw3d969YtNG3aFF9++WWuj8+cOROfffYZ5s2bh71796JChQoIDQ3FnTt3jMf069cPx48fR2RkJNatW4fff/8dL730krVOocjyO/fU1FQcPHgQH374IQ4ePIiffvoJUVFReOaZZ3IcO3nyZJPfg9dff90a4RfZ/b5zAOjUqZPJOa1YscLk8ZL4nd/vvLOfb2xsLBYsWACdToeePXuaHFfs71tRsbRq1UqNHDnSeD8zM1P5+vqq8PBwDaOyrKtXryoAaseOHcZ97du3V2+++aZ2QVnAhAkTVNOmTXN9LCEhQTk6OqpVq1YZ9508eVIBULt377ZShNbx5ptvqjp16ii9Xq+UKp3fNQC1evVq4329Xq+8vb3VrFmzjPsSEhKUs7OzWrFihVJKqRMnTigAav/+/cZjfvvtN6XT6dTly5etFntx3Xvuudm3b58CoC5evGjc5+/vrz755BPLBmdBuZ33oEGDVLdu3fJ8Tmn4zgvyfXfr1k099thjJvvM8X2zhaMY0tPT8ddff6Fjx47GfXZ2dujYsSN2796tYWSWlZiYCACoUqWKyf5ly5ahWrVqCAoKQlhYGFJTU7UIz6xOnz4NX19f1K5dG/369UN0dDQA4K+//kJGRobJdx8YGIiaNWuWqu8+PT0dS5cuxYsvvmiy2GFp/K6zO3/+POLi4ky+Xzc3N7Ru3dr4/e7evRvu7u544IEHjMd07NgRdnZ22Lt3r9VjtqTExETodDq4u7ub7J8+fTqqVq2K5s2bY9asWbh79642AZrR9u3b4enpiQYNGmDEiBG4fv268bGy8J3Hx8dj/fr1GDp0aI7Hivt9l7nF28zp2rVryMzMhJeXl8l+Ly8v/PPPPxpFZVl6vR6jRo1C27ZtERQUZNz/wgsvwN/fH76+vvj7778xduxYREVF4aefftIw2uJp3bo1IiIi0KBBA8TGxmLSpEl45JFHcOzYMcTFxcHJySnHf8BeXl6Ii4vTJmALWLNmDRISEjB48GDjvtL4Xd/L8B3m9m/b8FhcXBw8PT1NHndwcECVKlVK1e/AnTt3MHbsWPTt29dkMa833ngDLVq0QJUqVbBr1y6EhYUhNjYWc+bM0TDa4unUqRN69OiBWrVq4ezZs3jvvffQuXNn7N69G/b29mXiO1+0aBEqVaqUo3vYHN83Ew4qlJEjR+LYsWMmYxkAmPRhBgcHw8fHB48//jjOnj2LOnXqWDtMs+jcubPxdpMmTdC6dWv4+/tj5cqVKF++vIaRWc+3336Lzp07w9fX17ivNH7XlLuMjAw8//zzUErhq6++Mnns7bffNt5u0qQJnJyc8PLLLyM8PLzElgTv06eP8XZwcDCaNGmCOnXqYPv27Xj88cc1jMx6FixYgH79+qFcuXIm+83xfbNLpRiqVasGe3v7HDMT4uPj4e3trVFUlvPaa69h3bp12LZtG2rUqJHvsa1btwYAnDlzxhqhWYW7uzvq16+PM2fOwNvbG+np6UhISDA5pjR99xcvXsTmzZsxbNiwfI8rjd+14TvM79+2t7d3jsHhd+/exY0bN0rF74Ah2bh48SIiIyPvu0x769atcffuXVy4cME6AVpB7dq1Ua1aNePvdmn/zv/44w9ERUXd9988ULTvmwlHMTg5OaFly5bYsmWLcZ9er8eWLVsQEhKiYWTmpZTCa6+9htWrV2Pr1q2oVavWfZ9z+PBhAICPj4+Fo7OelJQUnD17Fj4+PmjZsiUcHR1NvvuoqChER0eXmu9+4cKF8PT0RJcuXfI9rjR+17Vq1YK3t7fJ95uUlIS9e/cav9+QkBAkJCTgr7/+Mh6zdetW6PV6YxJWUhmSjdOnT2Pz5s2oWrXqfZ9z+PBh2NnZ5ehyKMkuXbqE69evG3+3S/N3DkiLZsuWLdG0adP7Hluk77tYQ05Jfffdd8rZ2VlFRESoEydOqJdeekm5u7uruLg4rUMzmxEjRig3Nze1fft2FRsba9xSU1OVUkqdOXNGTZ48WR04cECdP39erV27VtWuXVu1a9dO48iL55133lHbt29X58+fV3/++afq2LGjqlatmrp69apSSqlXXnlF1axZU23dulUdOHBAhYSEqJCQEI2jNo/MzExVs2ZNNXbsWJP9pem7Tk5OVocOHVKHDh1SANScOXPUoUOHjDMxpk+frtzd3dXatWvV33//rbp166Zq1aqlbt++bXyNTp06qebNm6u9e/eqnTt3qnr16qm+fftqdUoFlt+5p6enq2eeeUbVqFFDHT582OTffFpamlJKqV27dqlPPvlEHT58WJ09e1YtXbpUeXh4qIEDB2p8ZvnL77yTk5PV6NGj1e7du9X58+fV5s2bVYsWLVS9evXUnTt3jK9REr/z+/2uK6VUYmKicnFxUV999VWO55vr+2bCYQaff/65qlmzpnJyclKtWrVSe/bs0TokswKQ67Zw4UKllFLR0dGqXbt2qkqVKsrZ2VnVrVtXjRkzRiUmJmobeDH17t1b+fj4KCcnJ1W9enXVu3dvdebMGePjt2/fVq+++qqqXLmycnFxUc8++6yKjY3VMGLz2bhxowKgoqKiTPaXpu9627Ztuf5eDxo0SCklU2M//PBD5eXlpZydndXjjz+e4/O4fv266tu3r6pYsaJydXVVQ4YMUcnJyRqcTeHkd+7nz5/P89/8tm3blFJK/fXXX6p169bKzc1NlStXTjVs2FBNmzbN5MJsi/I779TUVPXkk08qDw8P5ejoqPz9/dXw4cNz/PFYEr/z+/2uK6XU//73P1W+fHmVkJCQ4/nm+r65PD0RERFZHMdwEBERkcUx4SAiIiKLY8JBREREFseEg4iIiCyOCQcRERFZHBMOIiIisjgmHERERGRxTDiIiIjI4phwEFGJFBAQgLlz52odBhEVEBMOIrqvwYMHo3v37gCADh06YNSoUVZ774iICLi7u+fYv3//frz00ktWi4OIisdB6wCIqGxKT0+Hk5NTkZ/v4eFhxmiIyNLYwkFEBTZ48GDs2LEDn376KXQ6HXQ6HS5cuAAAOHbsGDp37oyKFSvCy8sLAwYMwLVr14zP7dChA1577TWMGjUK1apVQ2hoKABgzpw5CA4ORoUKFeDn54dXX30VKSkpAIDt27djyJAhSExMNL7fxIkTAeTsUomOjka3bt1QsWJFuLq64vnnn0d8fLzx8YkTJ6JZs2ZYsmQJAgIC4Obmhj59+iA5OdmyHxoRAWDCQUSF8OmnnyIkJATDhw9HbGwsYmNj4efnh4SEBDz22GNo3rw5Dhw4gA0bNiA+Ph7PP/+8yfMXLVoEJycn/Pnnn5g3bx4AwM7ODp999hmOHz+ORYsWYevWrXj33XcBAG3atMHcuXPh6upqfL/Ro0fniEuv16Nbt264ceMGduzYgcjISJw7dw69e/c2Oe7s2bNYs2YN1q1bh3Xr1mHHjh2YPn26hT4tIsqOXSpEVGBubm5wcnKCi4sLvL29jfu/+OILNG/eHNOmTTPuW7BgAfz8/HDq1CnUr18fAFCvXj3MnDnT5DWzjwcJCAjARx99hFdeeQX/93//BycnJ7i5uUGn05m83722bNmCo0eP4vz58/Dz8wMALF68GI0bN8b+/fvx4IMPApDEJCIiApUqVQIADBgwAFu2bMHUqVOL98EQ0X2xhYOIiu3IkSPYtm0bKlasaNwCAwMBSKuCQcuWLXM8d/PmzXj88cdRvXp1VKpUCQMGDMD169eRmppa4Pc/efIk/Pz8jMkGADRq1Aju7u44efKkcV9AQIAx2QAAHx8fXL16tVDnSkRFwxYOIiq2lJQUdO3aFTNmzMjxmI+Pj/F2hQoVTB67cOECnn76aYwYMQJTp05FlSpVsHPnTgwdOhTp6elwcXExa5yOjo4m93U6HfR6vVnfg4hyx4SDiArFyckJmZmZJvtatGiBH3/8EQEBAXBwKPh/K3/99Rf0ej1mz54NOztpcF25cuV93+9eDRs2RExMDGJiYoytHCdOnEBCQgIaNWpU4HiIyHLYpUJEhRIQEIC9e/fiwoULuHbtGvR6PUaOHIkbN26gb9++2L9/P86ePYuNGzdiyJAh+SYLdevWRUZGBj7//HOcO3cOS5YsMQ4mzf5+KSkp2LJlC65du5ZrV0vHjh0RHByMfv364eDBg9i3bx8GDhyI9u3b44EHHjD7Z0BEhceEg4gKZfTo0bC3t0ejRo3g4eGB6Oho+Pr64s8//0RmZiaefPJJBAcHY9SoUXB3dze2XOSmadOmmDNnDmbMmIGgoCAsW7YM4eHhJse0adMGr7zyCnr37g0PD48cg04B6RpZu3YtKleujHbt2qFjx46oXbs2vv/+e7OfPxEVjU4ppbQOgoiIiEo3tnAQERGRxTHhICIiIotjwkFEREQWx4SDiIiILI4JBxEREVkcEw4iIiKyOCYcREREZHFMOIiIiMjimHAQERGRxTHhICIiIotjwkFEREQW9/+ml1dX/oqPlAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "#\n",
    "#  Plot the metrics\n",
    "#\n",
    "# ---------------------------\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(info['iter_num'], info['loss']['train'], label=\"Train\", color='blue')\n",
    "plt.plot(info['iter_num'], info['loss']['val'], label=\"Val\", color='blue', ls='dashed')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
