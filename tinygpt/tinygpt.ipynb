{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "- The GPT2 paper: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "- The GPT3 paper: https://arxiv.org/pdf/2005.14165\n",
    "- The Transformer paper: https://arxiv.org/pdf/1706.03762\n",
    "- Based on Karpathy's tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3925s \n",
    "- Karpathy's nanoGPT https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset\n",
    "\n",
    "We use the  tinyshakespeare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ingenioso hidalgo don Quijote de la Mancha\n",
      "\n",
      "\n",
      "TASA\n",
      "\n",
      "Yo, Juan Gallo de Andrada, escribano de Cámara\n"
     ]
    }
   ],
   "source": [
    "text = open(\"./data/quijote.txt\", 'r', encoding='utf-8').read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tokenizer\n",
    "\n",
    "For now we will consider a character level tokenizer, that is one character encodes to one token, and the GPT has to predict the next caracter. The pros are that the vocabulary size is very small, but the input sequences, the context length, get very large. \n",
    "\n",
    "Later we can use another kind of tokenizer, for instance, per word based. We can use the Hugging Face library for this. Other tokenizers are for examlpe, the tiktoker (used by GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '¡', '«', '»', '¿', 'Á', 'É', 'Í', 'Ñ', 'Ó', 'Ú', 'à', 'á', 'é', 'í', 'ï', 'ñ', 'ó', 'ù', 'ú', 'ü']\n",
      "vocab size: 91\n"
     ]
    }
   ],
   "source": [
    "## Tokenizer\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"vocabulary: \", chars)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "# tokenizer: we encode/decode strings into integer arrays (keep it simple). GPT uses tiktoker\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda string : [char_to_idx[ch] for ch in string]\n",
    "decode = lambda array : ''.join([idx_to_char[i] for i in array])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 47, 1, 49, 47, 64, 47, 1, 63, 60, 56, 47]\n",
      "La casa roja\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"La casa roja\"))\n",
    "print(decode(encode(\"La casa roja\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=torch.Size([2097953]), data.dtype=torch.int64\n",
      "tensor([25, 57,  1, 55, 59, 53, 51, 59, 55, 60, 64, 60,  1, 54, 55, 50, 47, 57,\n",
      "        53, 60,  1, 50, 60, 59,  1, 36, 66, 55, 56, 60, 65, 51,  1, 50, 51,  1,\n",
      "        57, 47,  1, 32, 47, 59, 49, 54, 47,  0,  0,  0, 39, 21, 38, 21,  0,  0,\n",
      "        44, 60,  7,  1, 30, 66, 47, 59,  1, 27, 47, 57, 57, 60,  1, 50, 51,  1,\n",
      "        21, 59, 50, 63, 47, 50, 47,  7,  1, 51, 64, 49, 63, 55, 48, 47, 59, 60,\n",
      "         1, 50, 51,  1, 23, 82, 58, 47, 63, 47])\n"
     ]
    }
   ],
   "source": [
    "# Encode all data\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "print(f\"{data.shape=}, {data.dtype=}\")\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the train/validation splits\n",
    "\n",
    "The data is just a huge one dimensional tensor of integers. Let us split it in train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train.shape=torch.Size([1888157]), data_train.dtype=torch.int64\n",
      "data_val.shape=torch.Size([209796]), data_val.dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "# create the training/validation splits\n",
    "n = int(0.9 * len(data))\n",
    "data_train = data[:n]\n",
    "data_val = data[n:]\n",
    "print(f\"{data_train.shape=}, {data_train.dtype=}\")\n",
    "print(f\"{data_val.shape=}, {data_val.dtype=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the input and what is the target?\n",
    "\n",
    "- We consider sequences of data, for instance sentences. Usually it is called context length. The maximum of this size is fixed before hand.\n",
    "- Also, we pass the data in batches\n",
    "\n",
    "The target will be: given one sequence of characters, the next character will be the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "tensor([[ 1, 51, 59,  1, 57, 47, 64,  1],\n",
      "        [60,  9,  0, 35, 60, 63, 62, 66],\n",
      "        [51, 64, 65, 55, 58, 47, 50, 47],\n",
      "        [55, 59, 65, 51, 59, 49, 55, 87]]) \n",
      " tensor([[51, 59,  1, 57, 47, 64,  1, 65],\n",
      "        [ 9,  0, 35, 60, 63, 62, 66, 51],\n",
      "        [64, 65, 55, 58, 47, 50, 47,  1],\n",
      "        [59, 65, 51, 59, 49, 55, 87, 59]])\n",
      "When the input is [1] the target is 51\n",
      "When the input is [1, 51] the target is 59\n",
      "When the input is [1, 51, 59] the target is 1\n",
      "When the input is [1, 51, 59, 1] the target is 57\n",
      "When the input is [1, 51, 59, 1, 57] the target is 47\n",
      "When the input is [1, 51, 59, 1, 57, 47] the target is 64\n",
      "When the input is [1, 51, 59, 1, 57, 47, 64] the target is 1\n",
      "When the input is [1, 51, 59, 1, 57, 47, 64, 1] the target is 65\n"
     ]
    }
   ],
   "source": [
    "# get a batch of data. \n",
    "torch.manual_seed(1234)\n",
    "\n",
    "def get_batch(split : str, batch_size : str, block_size : str): # the input will be then a 4x8 tensor\n",
    "    dat = data_train if split == \"train\" else data_val\n",
    "    ix = torch.randint(0, dat.shape[0] - block_size, (batch_size,)) # create 4 random indeces\n",
    "    x =  torch.stack([dat[i:i+block_size] for i in ix])\n",
    "    y =  torch.stack([dat[i+1:i+block_size+1] for i in ix]) # It is shifted to the right\n",
    "    return x, y\n",
    "\n",
    "\n",
    "batch_size = 4 # how many sequencies (randomly generated) will fill the transformer at once\n",
    "block_size = 8 # the maximum context lenght for predictions\n",
    "xb, yb = get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "print(xb.shape, yb.shape)\n",
    "print(xb, \"\\n\", yb)\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When the input is {context.tolist()} the target is {target}\")\n",
    "    break\n",
    "\n",
    "# This effectevely contains a set of 8*4 independent examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape=torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "# Hyper params\n",
    "vocab_size = len(chars)\n",
    "block_size = 256\n",
    "n_embed = 384\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "max_iters = 5000\n",
    "eval_iters = 20\n",
    "eval_interval = 200\n",
    "dropout = 0.2\n",
    "\n",
    "##########\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "xb, yb = get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "print(f\"{xb.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding(xb).shape=torch.Size([64, 256, 384])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Token Embedding\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# For each token in the sequnce (a number within the range of vocab_size) will be assign a vector of size n_embed fom the embedding table.\n",
    "# This encodes the identity of each token\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size : int, n_embed : int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embed = n_embed\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # multiplied by sqrt(d_model) in the paper\n",
    "        return self.embedding(x) * np.sqrt(self.n_embed)  # (batch, block_size) -> (batch, block_size, n_embed)\n",
    "\n",
    "# Test\n",
    "token_embedding = TokenEmbedding(vocab_size=vocab_size, n_embed=n_embed)\n",
    "print(f\"{token_embedding(xb).shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 384])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Positional Embedding\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# We also encode the position\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, block_size : int, n_embed : int):\n",
    "        super().__init__()\n",
    "        self.context_length = block_size\n",
    "        self.n_embed = n_embed\n",
    "        self.embedding = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Here the input will be a tensor of the form (0, 1, 2, ... block_size)\n",
    "        return self.embedding(x) * np.sqrt(self.n_embed)  # (block_size) -> (block_size, n_embed)\n",
    "\n",
    "# Test\n",
    "positional_embedding = PositionalEmbedding(block_size=block_size, n_embed=n_embed)\n",
    "print(positional_embedding(torch.arange(block_size)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 91])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Projection Layer\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "#  This is just a linear layer that projects from embedding space to vocab space. It is applied at the end of the transformer\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, vocab_size : int, n_embed : int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embed = n_embed\n",
    "        self.projection_layer = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection_layer(x)   # (batch, block_size, n_embed) -> (batch, block_size, vocab_size)\n",
    "\n",
    "# Test\n",
    "projection_layer = ProjectionLayer(vocab_size=vocab_size, n_embed=n_embed)\n",
    "xt = torch.randn(batch_size, block_size, n_embed)\n",
    "print(projection_layer(xt).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Muli-Head Attention\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# 1. The multihead attention dividies the embedding dimension into multiple smaller attentions. \n",
    "# 2. The number of heads must divide the input dimension (the sequence one)\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_embed : int, num_heads : int, block_size : int , dropout : float):\n",
    "        super().__init__()\n",
    "        assert n_embed % num_heads == 0, \"num_heads must divide n_embed\" # check that n_embed can be divided by the num of heads\n",
    "        self.n_embed = n_embed\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = n_embed // num_heads\n",
    "\n",
    "        self.query = nn.Linear(n_embed, n_embed, bias=False) # Wq\n",
    "        self.key = nn.Linear(n_embed, n_embed, bias=False) # Wk\n",
    "        self.value = nn.Linear(n_embed, n_embed, bias=False) # Wv\n",
    "        self.proj = nn.Linear(n_embed, n_embed, bias=False) # Wo\n",
    "\n",
    "        self.register_buffer('causal_mask', torch.tril(torch.ones(block_size, block_size))) # (block_size, block_size) lower diagonal matrix\n",
    "   \n",
    "        self.dropout = None if dropout == None else nn.Dropout(dropout)\n",
    "        self.attention_scores = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, x_block_size, n_emb = x.shape\n",
    "        \n",
    "        q = self.query(x) # (batch, x_block_size, n_emb) @ (batch, n_emb, n_emb) -> (batch, x_block_size, n_emb)\n",
    "        k = self.key(x) \n",
    "        v = self.value(x) \n",
    "\n",
    "        # Split across the embedding dimension and rearange.\n",
    "        # (batch, x_block_size, n_emb) -> (batch, x_block_size, num_head, head_size) ->  (batch, num_head, x_block_size, head_size)\n",
    "        q = q.view(q.shape[0],  q.shape[1], self.num_heads, self.head_size).transpose(1,2) # (batch, num_head, x_block_size, head_size)\n",
    "        k = k.view(k.shape[0],  k.shape[1], self.num_heads, self.head_size).transpose(1,2)\n",
    "        v = v.view(v.shape[0],  v.shape[1], self.num_heads, self.head_size).transpose(1,2)\n",
    "\n",
    "        # Get the weights\n",
    "        weights = q @ k.transpose(-1, -2) / np.sqrt(self.head_size) # (batch, num_head, x_block_size, head_size) @  (batch, num_head, head_size, x_block_size) -> (batch, num_head, x_block_size, x_block_size)\n",
    "\n",
    "        # Causal mask \n",
    "        weights = weights.masked_fill(self.causal_mask[:x_block_size, :x_block_size] == 0, float('-inf')) # (batch, num_head, x_block_size, x_block_size)\n",
    "\n",
    "        # Apply softmax (the -inf will be 0)\n",
    "        weights = torch.softmax(weights, dim=-1) # sofmax each row (batch, num_heads, x_block_size, x_block_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        # if self.dropout is not None: weights = self.dropout(weights)\n",
    "        \n",
    "        # Finally multiply by the values\n",
    "        out =  weights @ v # (batch, num_head, x_block_size, x_block_size) @ (batch, num_head, x_block_size, head_size) -> (batch, num_head, x_block_size, head_size)\n",
    "        \n",
    "        # save the weights\n",
    "        self.attention_scores = weights\n",
    "        \n",
    "        # Concat the heads\n",
    "        # (batch, num_heads, x_block_size, head_size) -> (batch, x_block_size, num_heads, head_size) -> (batch, x_block_size, n_embed)\n",
    "        out = out.transpose(1,2).contiguous().view(out.shape[0], -1, self.num_heads * self.head_size) # (batch, x_block_size, n_embed). \n",
    "        \n",
    "        out = self.proj(out) # (batch, x_block_size, n_embed) @ (n_embed, n_embed) -> (batch, x_block_size, n_embed)\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout is not None: out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Feed Forward\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed : int,  dropout : float):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.ff1 = nn.Linear(n_embed, 4*n_embed) # hard code the hiden dim\n",
    "        self.ff2 = nn.Linear(4*n_embed, n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ff1(x)) # (batch, x_block_size, n_embed) -> (batch, x_block_size, 4 * n_embed)\n",
    "        x = self.ff2(x) # (batch, x_block_size, 4*n_embed) -> (batch, x_block_size, n_embed)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8, 2])\n",
      "tensor(0.7303)\n",
      "tensor(-3.7253e-09)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Layer Norm\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# Check the notes\n",
    "# TODO: check the mean(dim=-1). The sequence dimension is the dim = 1, no?\n",
    "# Normalized the rows instead of the columns (batch norm). This is taken from Karpathy, I have to check exaclty\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, eps=1e-6):\n",
    "    super().__init__()\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(-1, keepdim=True) # batch mean\n",
    "    xvar = x.var(-1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "\n",
    "# Test\n",
    "layer_norm = LayerNorm(2)\n",
    "out = layer_norm(torch.randn(16, 8, 2))\n",
    "print(out.shape)\n",
    "print(out[0, :].std()) # normalized across the context dimension\n",
    "print(out[0, :].mean()) # normalized across the context dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Transformer Layer\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, n_embed: int, num_heads: int, block_size : int,  dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttentionBlock(n_embed=n_embed, num_heads=num_heads, block_size=block_size, dropout=dropout)\n",
    "        self.feed_forward = FeedForward(n_embed=n_embed, dropout=dropout)\n",
    "        self.layer_norm1 = LayerNorm(n_embed)\n",
    "        self.layer_norm2 = LayerNorm(n_embed)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Just like this, it does not work, we get loss of 2.33, with 4 layers. Gradients might vanish\n",
    "        # x = self.self_attention(x)\n",
    "        # x = self.feed_forward(x) \n",
    "\n",
    "        # Add the residual connections + layer norms\n",
    "        x = x + self.self_attention(self.layer_norm1(x)) # (batch, x_block_size, n_embed) ->  (batch, x_block_size, n_embed)\n",
    "        x = x + self.feed_forward(self.layer_norm2(x)) # (batch, x_block_size, n_embed) ->  (batch, x_block_size, n_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size : int, n_embed : int, num_heads: int, num_layers: int,  block_size : int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size \n",
    "        self.n_embed = n_embed\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding = TokenEmbedding(vocab_size=vocab_size, n_embed=n_embed)\n",
    "        self.positional_embedding = PositionalEmbedding(block_size=block_size, n_embed=n_embed)\n",
    "\n",
    "        # attention\n",
    "        # self.sa_block = MultiHeadAttentionBlock(n_embed=n_embed, num_heads=num_heads, block_size=block_size, dropout=None)\n",
    "        # self.ff_block = FeedForward(n_embed=n_embed, hid_dim=4*n_embed, dropout=0.0)\n",
    "\n",
    "        self.layers = nn.Sequential( \n",
    "            *[ Layer(n_embed=n_embed, num_heads=num_heads, dropout=0.0) for _ in range(num_layers)] )\n",
    "\n",
    "\n",
    "        self.projection_layer = ProjectionLayer(vocab_size=vocab_size, n_embed=n_embed)\n",
    "\n",
    "        # Initialize the parameters\n",
    "        self._init_parameters()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, idx_block_size = idx.shape\n",
    "        # print(f\"{block_size=}\")\n",
    "\n",
    "        device = idx.device \n",
    "\n",
    "        tok_emb = self.token_embedding(idx) # (batch, block_size) -> (batch, block_size, n_embed)\n",
    "        pos_emb = self.positional_embedding(torch.arange(idx_block_size, device=device)) # (block_size, n_embed)\n",
    "        x = tok_emb + pos_emb # broadcasting works. (batch, block_size, n_embed)\n",
    "\n",
    "        # attention\n",
    "        # x = self.sa_block(x) # (batch, idx_block_size, n_embed) -> (batch, idx_block_size, n_embed)\n",
    "        # x = self.ff_block(x) # (batch, idx_block_size, n_embed)\n",
    "        x = self.layers(x) # (batch, idx_block_size, n_embed) -> (batch, idx_block_size, n_embed)\n",
    "\n",
    "\n",
    "        # the last layer\n",
    "        logits = self.projection_layer(x) # (batch, block_size, n_embed) -> (batch, block_size, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets != None: \n",
    "            # cross entropy expects the form (batch, classes). Essentially, each token in a sequence acts as an independent input. So we can group it with the batch \n",
    "            logits = logits.view(logits.shape[0] * idx_block_size, vocab_size) # (batch, block_size, vocab_size) -> (batch * block_size, vocab_size) \n",
    "            # The targets are of the form (batch, block_size)\n",
    "            targets = targets.view(targets.shape[0] * idx_block_size) # no need to create a one hot encoding. In this form, cross entropy does it for you (CHECK)\n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the idx so that only take the maximum block size.\n",
    "            idx_crop = idx[:, -self.block_size:]\n",
    "            # forward pass the model\n",
    "            logits, _ = self.forward(idx_crop) # (batch = 1 for 1 prediciton, block_size) -> (batch, block_size, vocab_size)\n",
    "            # get the last token, we want to predict the next token from the last (given all the context, etc...)\n",
    "            logits = logits[:, -1, :] # (batch, block_size, vocab_size) -> (batch, vocab_size)\n",
    "            # apply softmax to get a probability distribution across the vocabulary\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "\n",
    "            # Then we could get the token with most associated probability for the next token. But we can also sample from this\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch, 1)\n",
    "            \n",
    "            # Append the token to the previous one, that serves as a new context\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "\n",
    "    # Initiliazion of the parameters, as in the paper. I think PyTorch does it automatically, because all are linear layers. \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Generate text\n",
    "def write_text(model, max_tokens, decode, filename=None):\n",
    "    initial_context = torch.zeros((1, 1), dtype=torch.int64) # (batch_size = 1, block_size = 1)\n",
    "    if filename != None: f = open(filename, 'w')\n",
    "    print(decode( model.generate(initial_context, max_new_tokens=max_tokens)[0].tolist() ), file = None if filename == None else f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 91]) 8.133414268493652\n"
     ]
    }
   ],
   "source": [
    "# Test batch\n",
    "model = Transformer(vocab_size=vocab_size, n_embed=n_embed, num_heads=4, num_layers=4, block_size=block_size)\n",
    "model.to(device)\n",
    "xt, yt = get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "logitst, losst = model(xt, yt)\n",
    "print(logitst.shape, losst.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#\n",
    "# Training loop\n",
    "#\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "model = Transformer(vocab_size=vocab_size, n_embed=n_embed, num_heads=num_heads, num_layers=num_layers, block_size=block_size)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# The optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Estimate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size=batch_size, block_size=block_size)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test before training\n",
    "write_text(model, max_tokens=1000, decode=decode, filename='./output/random.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.6909, val loss 8.6736\n",
      "step 200: train loss 2.3514, val loss 2.3619\n",
      "step 400: train loss 2.3079, val loss 2.3128\n",
      "step 600: train loss 2.2234, val loss 2.2232\n",
      "step 800: train loss 2.0193, val loss 2.0335\n",
      "step 1000: train loss 1.8432, val loss 1.8593\n",
      "step 1200: train loss 1.7506, val loss 1.7587\n",
      "step 1400: train loss 1.6447, val loss 1.6652\n",
      "step 1600: train loss 1.5727, val loss 1.6028\n",
      "step 1800: train loss 1.5128, val loss 1.5332\n",
      "step 2000: train loss 1.4621, val loss 1.4933\n",
      "step 2200: train loss 1.4192, val loss 1.4588\n",
      "step 2400: train loss 1.3781, val loss 1.4159\n",
      "step 2600: train loss 1.3531, val loss 1.3924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#backward\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# first we set to zero the gradients\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# compute the gradients\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update the parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ai/deeplearning/venv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/ai/deeplearning/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/ai/deeplearning/venv/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for iter in range(max_iters):\n",
    " \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    # get batch\n",
    "    xb, yb = get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "    #forward\n",
    "    logits, loss = model(xb, yb)\n",
    "    #backward\n",
    "    optimizer.zero_grad(set_to_none=True) # first we set to zero the gradients\n",
    "    loss.backward() # compute the gradients\n",
    "    optimizer.step() # update the parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "checkpoint = { \"model\": model.state_dict(), \"optimizer\": optimizer.state_dict() }\n",
    "torch.save(checkpoint, \"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text(model, max_tokens=5000, decode=decode, filename='./output/trained.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and tests on the attention mechanishm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]), wei.shape=torch.Size([8, 8])\n",
      "x_out[0]=tensor([[ 5.4973e-02,  9.7783e-01],\n",
      "        [ 7.6479e-01,  1.2942e-01],\n",
      "        [ 6.9221e-01,  2.4644e-01],\n",
      "        [ 4.8400e-01,  2.6666e-02],\n",
      "        [ 2.3302e-01, -4.3245e-01],\n",
      "        [ 6.8370e-02,  1.3068e-01],\n",
      "        [ 2.4524e-01,  1.7378e-04],\n",
      "        [ 1.0304e-01, -8.5936e-02]]), x_out.shape=torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "B, T, D = 4, 8, 2 # batch, block_size, embeding_dimension (channels)\n",
    "x_in = torch.randn(B, T, D) # the values. It is like a private information of the token\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "wei = torch.zeros((T,T)) # This weights will be the dot product between the keys (what I have) and the queries (what I look for)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # make the tokens only interact with the past tokens \n",
    "wei = F.softmax(wei, dim=-1) # normalize it\n",
    "wei # this will be an interaction matrix between the tokens\n",
    "print(f\"{wei=}, {wei.shape=}\")\n",
    "x_out = wei @ x_in # x_in wil be the values\n",
    "print(f\"{x_out[0]=}, {x_out.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# \n",
    "# Head of Self-Attention\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# Receives a input with dimensions (batch, block_size, head_size) and outputs the same\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size : int, block_size : int):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(head_size, head_size, bias=False) # The Wq matrix, just a layer of interaction\n",
    "        self.key = nn.Linear(head_size, head_size, bias=False) # The Wk matrix\n",
    "        self.value = nn.Linear(head_size, head_size, bias=False) # THe Wv matrix\n",
    "        \n",
    "        # This is for decoder attention. This could be moved in the forward method\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, x_block_size, head_size = x.shape\n",
    "\n",
    "        q = self.query(x) # (batch, x_block_size, head_size) -> (batch, x_block_size, head_size) \n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # compute the weights\n",
    "        wei = q @ k.transpose(-2,-1) / np.sqrt(head_size) # (batch, x_block_size, head_size) @ (batch, head_size, block_size) -> (batch, x_block_size, block_size)\n",
    "\n",
    "        # mask the affinities, only interaction with past tokens\n",
    "        wei = wei.masked_fill(self.mask[:x_block_size, :x_block_size] == 0, float('-inf'))\n",
    "        \n",
    "        # normalize\n",
    "        wei = torch.softmax(wei, dim=-1) # softmax across the rows\n",
    "\n",
    "        out = wei @ v # (batch, x_block_size, x_block_size) @ (batch, x_block_size, head_size) -> (batch, x_block_size, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Test\n",
    "head = Head(head_size=16, block_size=block_size)\n",
    "xt = torch.randn(batch_size, block_size, 16)\n",
    "print(head(xt).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# \n",
    "# Muti Head of Self-Attention\n",
    "#\n",
    "# ------------------------------\n",
    "\n",
    "# Receives a input with dimensions (batch, block_size, head_size) and outputs the same\n",
    "\n",
    "class MuliHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads : int , head_size : int, block_size : int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size, block_size=block_size) for _ in range(num_heads)])      \n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)  # (batch, x_block_size, head_size) -> (batch, x_block_size, head_size * num_heads)\n",
    "\n",
    "# Test\n",
    "multihead = MuliHeadAttention(num_heads=4, head_size=16//4, block_size=block_size)\n",
    "xt = torch.randn(batch_size, block_size, 16//4)\n",
    "print(multihead(xt).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer Norm\n",
    "From the paper https://arxiv.org/pdf/1607.06450\n",
    "\n",
    "If we have a batch of size N, then for item in the batch, let's say sentences of seq_len size, then we compute the mean and the std of each one and normalize the element as\n",
    "\n",
    "$$ x_i' = \\gamma \\dfrac{x_i - \\mu_i}{\\sqrt{\\sigma_j^2 + \\epsilon}} + \\beta$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
